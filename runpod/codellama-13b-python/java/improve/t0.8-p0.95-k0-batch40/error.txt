AUTHOR="codellama"
MODEL_NAME="CodeLlama-13b-Python-hf"
max_length=1024

temperature=0.8
top_k=0
top_p=0.95
batch_size=40

BASE_DIR=./runpod/codellama-13b-python/java/improve/t$temperature-p$top_p-k$top_k-batch$batch_size
mkdir -p $BASE_DIR

n_samples=200
seed=0
precision=bf16
lang=java

limit_start=14
limit=11
eval_limit_start=0
eval_limit=50

save_every_k_tasks=1 # after completing k dataset's tasks
save_every_k_iterations=$((save_every_k_tasks * n_samples / batch_size))

common_name="$MODEL_NAME-temp$temperature-p$top_p-k$top_k-$precision-n$n_samples-batch$batch_size-maxlen$max_length-$lang"
generations_name="$common_name-generations-${limit_start}-${limit}_multiple-$lang"
generations_path="$BASE_DIR/$generations_name.json"

python main.py --model "$AUTHOR/$MODEL_NAME" \
    --tasks multiple-$lang \
    --max_length_generation $max_length \
    --temperature $temperature \
    --max_memory_per_gpu autot \SE_DIR/$common_name-generations-${limit_start}-${limit}.json" \
Selected Tasks: ['multiple-java']
Loading model in bf16
Loading model in auto mode
config.json: 100%|█████████████████████████████████████████████████████████████████████| 589/589 [00:00<00:00, 4.88MB/s]
model.safetensors.index.json: 100%|████████████████████████████████████████████████| 31.4k/31.4k [00:00<00:00, 14.8MB/s]
model-00001-of-00003.safetensors: 100%|█████████████████████████████████████████████| 9.95G/9.95G [01:26<00:00, 115MB/s]
model-00002-of-00003.safetensors: 100%|█████████████████████████████████████████████| 9.90G/9.90G [01:26<00:00, 115MB/s]
model-00003-of-00003.safetensors: 100%|█████████████████████████████████████████████| 6.18G/6.18G [00:57<00:00, 108MB/s]
Downloading shards: 100%|█████████████████████████████████████████████████████████████████| 3/3 [03:50<00:00, 76.77s/it]
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████| 3/3 [00:05<00:00,  1.74s/it]
generation_config.json: 100%|██████████████████████████████████████████████████████████| 116/116 [00:00<00:00, 1.92MB/s]
tokenizer_config.json: 100%|███████████████████████████████████████████████████████████| 749/749 [00:00<00:00, 9.27MB/s]
tokenizer.model: 100%|████████████████████████████████████████████████████████████████| 500k/500k [00:00<00:00, 118MB/s]
tokenizer.json: 100%|██████████████████████████████████████████████████████████████| 1.84M/1.84M [00:00<00:00, 21.8MB/s]
special_tokens_map.json: 100%|█████████████████████████████████████████████████████████| 411/411 [00:00<00:00, 4.05MB/s]
generation only mode
Downloading builder script: 100%|██████████████████████████████████████████████████| 4.05k/4.05k [00:00<00:00, 26.4MB/s]
Downloading metadata: 100%|██████████████████████████████████████████████████████████| 478k/478k [00:00<00:00, 10.5MB/s]
Downloading readme: 100%|██████████████████████████████████████████████████████████| 99.6k/99.6k [00:00<00:00, 4.50MB/s]
Downloading data: 321kB [00:00, 182MB/s]                                                                                
Generating test split: 100%|████████████████████████████████████████████████| 158/158 [00:00<00:00, 12783.07 examples/s]
number of problems for this task is 11
task range: 15->25
200 completions required for each task
40 completion/prompt
5 batch/task
55 batches (iterations) required for 11 tasks
  0%|                                                                                            | 0/55 [00:12<?, ?it/s]
Traceback (most recent call last):
  File "/workspace/bigcode-evaluation-harness/main.py", line 431, in <module>
    main()
  File "/workspace/bigcode-evaluation-harness/main.py", line 399, in main
    generations, references = evaluator.generate_text(
  File "/workspace/bigcode-evaluation-harness/bigcode_eval/evaluator.py", line 69, in generate_text
    generations = parallel_generations(
  File "/workspace/bigcode-evaluation-harness/bigcode_eval/generation.py", line 155, in parallel_generations
    generations = complete_code(
  File "/workspace/bigcode-evaluation-harness/bigcode_eval/utils.py", line 310, in complete_code
    generated_tokens = model.generate(
  File "/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py", line 1989, in generate
    result = self._sample(
  File "/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py", line 2969, in _sample
    next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)
RuntimeError: probability tensor contains either `inf`, `nan` or element < 0