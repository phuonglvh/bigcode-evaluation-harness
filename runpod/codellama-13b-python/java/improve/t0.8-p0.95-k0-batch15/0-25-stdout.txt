AUTHOR="codellama"
root@C.11775330:/workspace/bigcode-evaluation-harness$ AUTHOR="codellama"
root@C.11775330:/workspace/bigcode-evaluation-harness$ AUTHOR="codellama"
root@C.11775330:/workspace/bigcode-evaluation-harness$ AUTHOR="codellama"
root@C.11775330:/workspace/bigcode-evaluation-harness$ AUTHOR="codellama"
root@C.11775330:/workspace/bigcode-evaluation-harness$ AUTHOR="codellama"
root@C.11775330:/workspace/bigcode-evaluation-harness$ AUTHOR="codellama"
root@C.11775330:/workspace/bigcode-evaluation-harness$ AUTHOR="codellama"
root@C.11775330:/workspace/bigcode-evaluation-harness$ AUTHOR="codellama"
MODEL_NAME="CodeLlama-13b-Python-hf"on/java/improve/t$temperature-p$top_p-k$top_k-batch$batch_size
max_length=1024od/codellama-13b-python/java/improve/t$temperature-p$top_p-k$top_k-batch$batch_size
temperature=0.8od/codellama-13b-python/java/improve/t$temperature-p$top_p-k$top_k-batch$batch_size
temperature=0.8od/codellama-13b-python/java/improve/t$temperature-p$top_p-k$top_k-batch$batch_size
top_p=0.95=15npod/codellama-13b-python/java/improve/t$temperature-p$top_p-k$top_k-batch$batch_size
top_k=0ize=15npod/codellama-13b-python/java/improve/t$temperature-p$top_p-k$top_k-batch$batch_size
batch_size=15npod/codellama-13b-python/java/improve/t$temperature-p$top_p-k$top_k-batch$batch_size
BASE_DIR=./runpod/codellama-13b-python/java/improve/t$temperature-p$top_p-k$top_k-batch$batch_size
BASE_DIR=./runpod/codellama-13b-python/java/improve/t$temperature-p$top_p-k$top_k-batch$batch_size
mkdir -p $BASE_DIR
n_samples=2006rt=0
n_samples=2006rt=0
seed=0ion=bf16rt=0
precision=bf16rt=0=1 # after completing k dataset's tasks
lang=javart=0art=0=1 # after completing k dataset's tasks / batch_size))
limit_start=0art=0=1 # after completing k dataset's tasks / batch_size))
limit_start=0art=0=1 # after completing k dataset's tasks / batch_size))_samples-batch$batch_size-maxlen$max_len
limit=25it_start=0=1 # after completing k dataset's tasks / batch_size))_samples-batch$batch_size-maxlen$max_len
eval_limit_start=0=1 # after completing k dataset's tasks / batch_size))_samples-batch$batch_size-maxlen$max_leng
eval_limit=50tasks=1 # after completing k dataset's tasks / batch_size))_samples-batch$batch_size-maxlen$max_length
save_every_k_tasks=1 # after completing k dataset's tasks / batch_size))_samples-batch$batch_size-maxlen$max_length
save_every_k_tasks=1 # after completing k dataset's tasks / batch_size))_samples-batch$batch_size-maxlen$max_length-
save_every_k_iterations=$((save_every_k_tasks * n_samples / batch_size))_samples-batch$batch_size-maxlen$max_length-$
common_name="$MODEL_NAME-temp$temperature-p$top_p-k$top_k-$precision-n$n_samples-batch$batch_size-maxlen$max_length-$
common_name="$MODEL_NAME-temp$temperature-p$top_p-k$top_k-$precision-n$n_samples-batch$batch_size-maxlen$max_length-$lang"ations_name="$common_name-generations-${limit_start}-${limit}_multiple-$lang"
generations_name="$common_name-generations-${limit_start}-${limit}_multiple-$lang"
generations_path="$BASE_DIR/$generations_name.json"
python main.py --model "$AUTHOR/$MODEL_NAME" \
python main.py --model "$AUTHOR/$MODEL_NAME" \
    --tasks multiple-$lang \$max_length \
    --max_length_generation $max_length \
    --temperature $temperature \
    --max_memory_per_gpu autot \SE_DIR/$common_name-generations-${limit_start}-${limit}.json" \
Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Selected Tasks: ['multiple-java']
Loading model in bf16
Loading model in auto mode
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████| 3/3 [00:03<00:00,  1.15s/it]
generation only mode
number of problems for this task is 25
task range: 1->25
200 completions required for each task
15 completion/prompt
14 batch/task
350 batches (iterations) required for 25 tasks
  3%|██▋                                                                            | 12/350 [00:32<13:50,  2.46s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0-batch15/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch15-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
  7%|█████▌                                                                       | 25/350 [07:47<3:08:06, 34.73s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0-batch15/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch15-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
 11%|████████▎                                                                    | 38/350 [11:26<1:28:50, 17.09s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0-batch15/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch15-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
 15%|███████████▌                                                                   | 51/350 [14:07<45:53,  9.21s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0-batch15/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch15-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
 17%|█████████████▍                                                               | 61/350 [15:54<1:02:31, 12.98s/it]
 18%|██████████████                                                               | 64/350 [16:35<1:03:15, 13.27s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0-batch15/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch15-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
 22%|█████████████████▍                                                             | 77/350 [19:40<59:53, 13.16s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0-batch15/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch15-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
 26%|████████████████████▎                                                          | 90/350 [22:58<47:56, 11.06s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0-batch15/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch15-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
 29%|██████████████████████▉                                                       | 103/350 [25:29<46:06, 11.20s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0-batch15/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch15-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
 33%|█████████████████████████▊                                                    | 116/350 [27:59<40:54, 10.49s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0-batch15/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch15-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
 37%|████████████████████████████▋                                                 | 129/350 [30:28<53:24, 14.50s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0-batch15/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch15-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
 41%|███████████████████████████████▋                                              | 142/350 [33:52<44:29, 12.83s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0-batch15/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch15-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
 44%|██████████████████████████████████▌                                           | 155/350 [35:57<25:19,  7.79s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0-batch15/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch15-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
 48%|█████████████████████████████████████▍                                        | 168/350 [37:08<16:00,  5.28s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0-batch15/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch15-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
 52%|████████████████████████████████████████▎                                     | 181/350 [37:31<04:46,  1.69s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0-batch15/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch15-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
 55%|███████████████████████████████████████████▏                                  | 194/350 [40:19<37:06, 14.27s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0-batch15/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch15-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
 59%|████████████████████████████████████████████▉                               | 207/350 [47:24<1:24:10, 35.32s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0-batch15/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch15-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
 63%|███████████████████████████████████████████████▊                            | 220/350 [56:52<1:44:32, 48.25s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0-batch15/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch15-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
 67%|██████████████████████████████████████████████████▌                         | 233/350 [1:00:53<19:10,  9.83s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0-batch15/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch15-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
 70%|████████████████████████████████████████████████████                      | 246/350 [1:06:35<1:12:56, 42.08s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0-batch15/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch15-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
 74%|████████████████████████████████████████████████████████▏                   | 259/350 [1:12:06<25:20, 16.71s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0-batch15/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch15-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
 78%|███████████████████████████████████████████████████████████                 | 272/350 [1:16:06<25:36, 19.70s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0-batch15/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch15-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
 81%|█████████████████████████████████████████████████████████████▉              | 285/350 [1:19:23<10:34,  9.77s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0-batch15/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch15-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
 85%|████████████████████████████████████████████████████████████████▋           | 298/350 [1:22:11<15:40, 18.09s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0-batch15/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch15-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
 89%|███████████████████████████████████████████████████████████████████▌        | 311/350 [1:27:11<08:57, 13.79s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0-batch15/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch15-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
 93%|██████████████████████████████████████████████████████████████████████▎     | 324/350 [1:29:42<11:35, 26.76s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0-batch15/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch15-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
 96%|█████████████████████████████████████████████████████████████████████████▏  | 337/350 [1:34:16<04:11, 19.32s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0-batch15/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch15-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
100%|████████████████████████████████████████████████████████████████████████████| 350/350 [1:40:01<00:00, 17.15s/it]
/workspace/bigcode-evaluation-harness/bigcode_eval/evaluator.py:85: UserWarning: Number of tasks wasn't proportional to number of devices, we removed extra predictions to only keep nsamples=200
  warnings.warn(
audit_generations: verifying generations against dataset
audit_generations: unknown_tasks []
generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0-batch15/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch15-maxlen1024-java-generations-0-25_multiple-java.json
references were saved at references_multiple-java.json
evaluation results:
{
  "config": {
    "prefix": "",
    "do_sample": true,
    "temperature": 0.8,
    "top_k": 0,
    "top_p": 0.95,
    "n_samples": 200,
    "eos": "<|endoftext|>",
    "seed": 0,
    "model": "codellama/CodeLlama-13b-Python-hf",
    "modeltype": "causal",
    "peft_model": null,
    "revision": null,
    "token": false,
    "trust_remote_code": true,
    "tasks": "multiple-java",
    "instruction_tokens": null,
    "batch_size": 15,
    "max_length_generation": 1024,
    "precision": "bf16",
    "load_in_8bit": false,
    "load_in_4bit": false,
    "left_padding": false,
    "limit": 25,
    "limit_start": 0,
    "save_every_k_tasks": 13,
    "postprocess": true,
    "allow_code_execution": true,
    "generation_only": true,
    "load_generations_path": null,
    "load_data_path": null,
    "metric_output_path": "evaluation_results.json",
    "save_generations": true,
    "load_generations_intermediate_paths": null,
    "save_generations_path": "./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0-batch15/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch15-maxlen1024-java-generations-0-25.json",
    "save_references": true,
    "save_references_path": "references.json",
    "prompt": "prompt",
    "max_memory_per_gpu": "auto",
    "check_references": false
  }
}