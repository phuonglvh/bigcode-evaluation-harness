root@C.11775330:/workspace/bigcode-evaluation-harness$ AUTHOR="codellama"
MODEL_NAME="CodeLlama-13b-Python-hf"
max_length=1024

temperature=0.8
top_k=0
top_p=0.95
batch_size=25

BASE_DIR=./runpod/codellama-13b-python/java/improve/t$temperature-p$top_p-k$top_k-batch$batch_size
mkdir -p $BASE_DIR

n_samples=200
seed=0
precision=bf16
lang=java

limit_start=0
limit=5
eval_limit_start=0
eval_limit=50

save_every_k_tasks=1 # after completing k dataset's tasks
save_every_k_iterations=$((save_every_k_tasks * n_samples / batch_size))

common_name="$MODEL_NAME-temp$temperature-p$top_p-k$top_k-$precision-n$n_samples-batch$batch_size-maxlen$max_length-$lang"
generations_name="$common_name-generations-${limit_start}-${limit}_multiple-$lang"
generations_path="$BASE_DIR/$generations_name.json"

python main.py --model "$AUTHOR/$MODEL_NAME" \
    --tasks multiple-$lang \
    --max_length_generation $max_length \
    --temperature $temperature \
    --max_memory_per_gpu autot \SE_DIR/$common_name-generations-${limit_start}-${limit}.json" \
Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Selected Tasks: ['multiple-java']
Loading model in bf16
Loading model in auto mode
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.00it/s]
generation only mode
number of problems for this task is 5
task range: 1->5
200 completions required for each task
25 completion/prompt
8 batch/task
40 batches (iterations) required for 5 tasks
 18%|██████████████▏                                                                  | 7/40 [00:27<01:54,  3.48s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0-batch25/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch25-maxlen1024-java-generations-0-5_multiple-java_intermediate.json
 28%|██████████████████████                                                          | 11/40 [05:26<14:19, 29.64s/it]
Traceback (most recent call last):
  File "/workspace/bigcode-evaluation-harness/main.py", line 431, in <module>
    main()
  File "/workspace/bigcode-evaluation-harness/main.py", line 399, in main
    generations, references = evaluator.generate_text(
  File "/workspace/bigcode-evaluation-harness/bigcode_eval/evaluator.py", line 69, in generate_text
    generations = parallel_generations(
  File "/workspace/bigcode-evaluation-harness/bigcode_eval/generation.py", line 155, in parallel_generations
    generations = complete_code(
  File "/workspace/bigcode-evaluation-harness/bigcode_eval/utils.py", line 310, in complete_code
    generated_tokens = model.generate(
  File "/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py", line 1989, in generate
    result = self._sample(
  File "/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py", line 2932, in _sample
    outputs = self(**model_inputs, return_dict=True)
  File "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py", line 169, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1141, in forward
    outputs = self.model(
  File "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 944, in forward
    layer_outputs = decoder_layer(
  File "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py", line 169, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 677, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
  File "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py", line 169, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 583, in forward
    key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)
  File "/opt/conda/lib/python3.10/site-packages/transformers/cache_utils.py", line 364, in update
    self.value_cache[layer_idx] = torch.cat([self.value_cache[layer_idx], value_states], dim=-2)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 246.00 MiB. GPU 0 has a total capacity of 23.68 GiB of which 226.69 MiB is free. Process 431207 has 23.46 GiB memory in use. Of the allocated memory 21.74 GiB is allocated by PyTorch, and 1.41 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
root@C.11775330:/workspace/bigcode-evaluation-harness$ python main.py --model "$AUTHOR/$MODEL_NAME"     --tasks multiple-$lang     --max_length_generation $max_length     --temperature $temperature     --top_p $top_p     --top_k $top_k     --seed $seed     --n_samples $n_samples     --batch_size $batch_size     --precision $precision     --allow_code_execution     --trust_remote_code     --save_every_k_tasks $save_every_k_iterations     --save_generations     --save_generations_path "$BASE_DIR/$common_name-generations-${limit_start}-${limit}.json"     --save_references     --generation_only     --limit_start $limit_start     --limit $limit     --max_memory_per_gpu auto --load_generations_path ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0-batch25/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch25-maxlen1024-java-generations-0-5_multiple-java_intermediate.json
/workspace/bigcode-evaluation-harness/main.py:238: UserWarning: evaluation only mode but results will not be saved at evaluation_results.json due to args.generation_only is True
  warnings.warn(
Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Selected Tasks: ['multiple-java']
evaluation only mode
loading generations from "/workspace/bigcode-evaluation-harness/runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0-batch25/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch25-maxlen1024-java-generations-0-5_multiple-java_intermediate.json"
generations loaded, 5 selected from 5 with 200 candidates
loaded tasks (0-based indexing): 0 - 5
audit_generations: verifying generations against dataset
Traceback (most recent call last):
  File "/workspace/bigcode-evaluation-harness/main.py", line 431, in <module>
    main()
  File "/workspace/bigcode-evaluation-harness/main.py", line 263, in main
    results[task] = evaluator.evaluate(task)
  File "/workspace/bigcode-evaluation-harness/bigcode_eval/evaluator.py", line 100, in evaluate
    generations, references = self.generate_text(task_name, intermediate_generations=intermediate_generations)
  File "/workspace/bigcode-evaluation-harness/bigcode_eval/evaluator.py", line 91, in generate_text
    task.audit_generations(generations)
  File "/workspace/bigcode-evaluation-harness/bigcode_eval/tasks/multiple.py", line 216, in audit_generations
    unknown_tasks = detect_unknown_tasks(
  File "/workspace/bigcode-evaluation-harness/bigcode_eval/tasks/multiple.py", line 223, in detect_unknown_tasks
    return java_detect_unknown_tasks(dataset, generations, start_idx)
  File "/workspace/bigcode-evaluation-harness/utils/java.py", line 21, in java_detect_unknown_tasks
    first_gen = task_gens[0]
IndexError: list index out of range
root@C.11775330:/workspace/bigcode-evaluation-harness$ AUTHOR="codellama"
MODEL_NAME="CodeLlama-13b-Python-hf"
max_length=1024

temperature=0.8
top_k=0
top_p=0.95
batch_size=25

BASE_DIR=./runpod/codellama-13b-python/java/improve/t$temperature-p$top_p-k$top_k-batch$batch_size
mkdir -p $BASE_DIR

n_samples=200
seed=0
precision=bf16
lang=java

limit_start=1
limit=4
eval_limit_start=0
eval_limit=50

save_every_k_tasks=1 # after completing k dataset's tasks
save_every_k_iterations=$((save_every_k_tasks * n_samples / batch_size))

common_name="$MODEL_NAME-temp$temperature-p$top_p-k$top_k-$precision-n$n_samples-batch$batch_size-maxlen$max_length-$lang"
generations_name="$common_name-generations-${limit_start}-${limit}_multiple-$lang"
generations_path="$BASE_DIR/$generations_name.json"

python main.py --model "$AUTHOR/$MODEL_NAME" \
    --tasks multiple-$lang \
    --max_length_generation $max_length \
    --temperature $temperature \
    --max_memory_per_gpu autot \SE_DIR/$common_name-generations-${limit_start}-${limit}.json" \
Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Selected Tasks: ['multiple-java']
Loading model in bf16
Loading model in auto mode
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.05it/s]
generation only mode
number of problems for this task is 4
task range: 2->5
200 completions required for each task
25 completion/prompt
8 batch/task
32 batches (iterations) required for 4 tasks
 22%|█████████████████▋                                                               | 7/32 [08:08<22:54, 55.00s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0-batch25/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch25-maxlen1024-java-generations-1-4_multiple-java_intermediate.json
 47%|█████████████████████████████████████▌                                          | 15/32 [12:27<10:42, 37.78s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0-batch25/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch25-maxlen1024-java-generations-1-4_multiple-java_intermediate.json
 72%|█████████████████████████████████████████████████████████▌                      | 23/32 [13:58<01:45, 11.71s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0-batch25/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch25-maxlen1024-java-generations-1-4_multiple-java_intermediate.json
 97%|█████████████████████████████████████████████████████████████████████████████▌  | 31/32 [16:40<00:19, 19.30s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0-batch25/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch25-maxlen1024-java-generations-1-4_multiple-java_intermediate.json
100%|████████████████████████████████████████████████████████████████████████████████| 32/32 [17:09<00:00, 32.17s/it]
audit_generations: verifying generations against dataset
audit_generations: unknown_tasks []
generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0-batch25/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch25-maxlen1024-java-generations-1-4_multiple-java.json
references were saved at references_multiple-java.json
evaluation results:
{
  "config": {
    "prefix": "",
    "do_sample": true,
    "temperature": 0.8,
    "top_k": 0,
    "top_p": 0.95,
    "n_samples": 200,
    "eos": "<|endoftext|>",
    "seed": 0,
    "model": "codellama/CodeLlama-13b-Python-hf",
    "modeltype": "causal",
    "peft_model": null,
    "revision": null,
    "token": false,
    "trust_remote_code": true,
    "tasks": "multiple-java",
    "instruction_tokens": null,
    "batch_size": 25,
    "max_length_generation": 1024,
    "precision": "bf16",
    "load_in_8bit": false,
    "load_in_4bit": false,
    "left_padding": false,
    "limit": 4,
    "limit_start": 1,
    "save_every_k_tasks": 8,
    "postprocess": true,
    "allow_code_execution": true,
    "generation_only": true,
    "load_generations_path": null,
    "load_data_path": null,
    "metric_output_path": "evaluation_results.json",
    "save_generations": true,
    "load_generations_intermediate_paths": null,
    "save_generations_path": "./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0-batch25/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch25-maxlen1024-java-generations-1-4.json",
    "save_references": true,
    "save_references_path": "references.json",
    "prompt": "prompt",
    "max_memory_per_gpu": "auto",
    "check_references": false
  }
}