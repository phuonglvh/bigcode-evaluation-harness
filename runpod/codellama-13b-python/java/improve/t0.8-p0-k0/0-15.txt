AUTHOR="codellama"
MODEL_NAME="CodeLlama-13b-Python-hf"
max_length=1024

temperature=0.8
top_p=0
top_k=0

BASE_DIR=./runpod/codellama-13b-python/java/improve/t$temperature-p$top_p-k$top_k
mkdir -p $BASE_DIR

n_samples=200
seed=0
precision=bf16
lang=java
batch_size=10

limit_start=0
limit=15
eval_limit_start=0
eval_limit=50

save_every_k_tasks=1 # after completing k dataset's tasks
save_every_k_iterations=$(($save_every_k_tasks*$n_samples/$batch_size))

common_name="$MODEL_NAME-temp$temperature-p$top_p-k$top_k-$precision-n$n_samples-batch$batch_size-maxlen$max_length-$lang"
generations_name="$common_name-generations-${limit_start}-${limit}_multiple-$lang"
generations_path="$BASE_DIR/$generations_name.json"
root@C.9150644:/workspace/bigcode-evaluation-harness$ python main.py --model "$AUTHOR/$MODEL_NAME" \
    --tasks multiple-$lang \
    --max_length_generation $max_length \
    --temperature $temperature \
    --top_p $top_p \
    --top_k $top_k \
    --seed $seed \
    --n_samples $n_samples \
    --batch_size $batch_size \
    --precision $precision \
    --allow_code_execution \
    --trust_remote_code \
    --save_every_k_tasks $save_every_k_iterations \
    --save_generations \
    --save_generations_path "$BASE_DIR/$common_name-generations-${limit_start}-${limit}.json" \
    --save_references \
    --generation_only \
    --limit_start $limit_start \
    --limit $limit
Selected Tasks: ['multiple-java']
Loading model in bf16
config.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 589/589 [00:00<00:00, 1.86MB/s]
model.safetensors.index.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 31.4k/31.4k [00:00<00:00, 74.2MB/s]
model-00001-of-00003.safetensors: 100%|██████████████████████████████████████████████████████████████████████████████████████| 9.95G/9.95G [02:51<00:00, 58.0MB/s]
model-00002-of-00003.safetensors: 100%|██████████████████████████████████████████████████████████████████████████████████████| 9.90G/9.90G [02:57<00:00, 55.9MB/s]
model-00003-of-00003.safetensors: 100%|██████████████████████████████████████████████████████████████████████████████████████| 6.18G/6.18G [01:47<00:00, 57.4MB/s]
Downloading shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [07:37<00:00, 152.42s/it]
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:03<00:00,  1.31s/it]
generation_config.json: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 116/116 [00:00<00:00, 695kB/s]
tokenizer_config.json: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 749/749 [00:00<00:00, 6.06MB/s]
tokenizer.model: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 500k/500k [00:00<00:00, 9.33MB/s]
tokenizer.json: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.84M/1.84M [00:00<00:00, 29.5MB/s]
special_tokens_map.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 411/411 [00:00<00:00, 3.71MB/s]
generation only mode
/opt/conda/lib/python3.10/site-packages/datasets/load.py:1429: FutureWarning: The repository for nuprl/MultiPL-E contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/nuprl/MultiPL-E
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
Downloading builder script: 100%|████████████████████████████████████████████████████████████████████████████████████████████| 4.05k/4.05k [00:00<00:00, 24.9MB/s]
Downloading metadata: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 478k/478k [00:00<00:00, 16.0MB/s]
Downloading readme: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 99.6k/99.6k [00:00<00:00, 3.65MB/s]
Downloading data: 321kB [00:00, 62.9MB/s]                                                                                                                         
Generating test split: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 158/158 [00:00<00:00, 9618.29 examples/s]
Downloading builder script: 100%|████████████████████████████████████████████████████████████████████████████████████████████| 4.05k/4.05k [00:00<00:00, 34.9MB/s]
Downloading metadata: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 478k/478k [00:00<00:00, 16.2MB/s]
Downloading readme: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 99.6k/99.6k [00:00<00:00, 7.00MB/s]
number of problems for this task is 15
task range: 1->15
200 completions required for each task
10 completion/prompt
20 batch/task
300 batches (iterations) required for 15 tasks
  6%|███████▊                                                                                                                    | 19/300 [00:15<03:42,  1.26it/s]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0-k0/CodeLlama-13b-Python-hf-temp0.8-p0-k0-bf16-n200-batch10-maxlen1024-java-generations-0-15_multiple-java_intermediate.json
 13%|███████████████▊                                                                                                          | 39/300 [05:21<1:09:39, 16.01s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0-k0/CodeLlama-13b-Python-hf-temp0.8-p0-k0-bf16-n200-batch10-maxlen1024-java-generations-0-15_multiple-java_intermediate.json
 20%|████████████████████████▍                                                                                                   | 59/300 [07:50<28:08,  7.01s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0-k0/CodeLlama-13b-Python-hf-temp0.8-p0-k0-bf16-n200-batch10-maxlen1024-java-generations-0-15_multiple-java_intermediate.json
 26%|████████████████████████████████▋                                                                                           | 79/300 [09:21<16:16,  4.42s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0-k0/CodeLlama-13b-Python-hf-temp0.8-p0-k0-bf16-n200-batch10-maxlen1024-java-generations-0-15_multiple-java_intermediate.json
 33%|████████████████████████████████████████▉                                                                                   | 99/300 [11:24<20:50,  6.22s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0-k0/CodeLlama-13b-Python-hf-temp0.8-p0-k0-bf16-n200-batch10-maxlen1024-java-generations-0-15_multiple-java_intermediate.json
 40%|████████████████████████████████████████████████▊                                                                          | 119/300 [13:22<17:42,  5.87s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0-k0/CodeLlama-13b-Python-hf-temp0.8-p0-k0-bf16-n200-batch10-maxlen1024-java-generations-0-15_multiple-java_intermediate.json
 46%|████████████████████████████████████████████████████████▉                                                                  | 139/300 [14:46<11:06,  4.14s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0-k0/CodeLlama-13b-Python-hf-temp0.8-p0-k0-bf16-n200-batch10-maxlen1024-java-generations-0-15_multiple-java_intermediate.json
 53%|█████████████████████████████████████████████████████████████████▏                                                         | 159/300 [16:15<10:25,  4.44s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0-k0/CodeLlama-13b-Python-hf-temp0.8-p0-k0-bf16-n200-batch10-maxlen1024-java-generations-0-15_multiple-java_intermediate.json
 60%|█████████████████████████████████████████████████████████████████████████▍                                                 | 179/300 [17:17<06:06,  3.03s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0-k0/CodeLlama-13b-Python-hf-temp0.8-p0-k0-bf16-n200-batch10-maxlen1024-java-generations-0-15_multiple-java_intermediate.json
 66%|█████████████████████████████████████████████████████████████████████████████████▌                                         | 199/300 [19:39<12:18,  7.31s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0-k0/CodeLlama-13b-Python-hf-temp0.8-p0-k0-bf16-n200-batch10-maxlen1024-java-generations-0-15_multiple-java_intermediate.json
 73%|█████████████████████████████████████████████████████████████████████████████████████████▊                                 | 219/300 [20:21<02:30,  1.85s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0-k0/CodeLlama-13b-Python-hf-temp0.8-p0-k0-bf16-n200-batch10-maxlen1024-java-generations-0-15_multiple-java_intermediate.json
 80%|█████████████████████████████████████████████████████████████████████████████████████████████████▉                         | 239/300 [21:31<03:38,  3.58s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0-k0/CodeLlama-13b-Python-hf-temp0.8-p0-k0-bf16-n200-batch10-maxlen1024-java-generations-0-15_multiple-java_intermediate.json
 86%|██████████████████████████████████████████████████████████████████████████████████████████████████████████▏                | 259/300 [21:51<00:34,  1.18it/s]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0-k0/CodeLlama-13b-Python-hf-temp0.8-p0-k0-bf16-n200-batch10-maxlen1024-java-generations-0-15_multiple-java_intermediate.json
 93%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍        | 279/300 [24:05<02:25,  6.95s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0-k0/CodeLlama-13b-Python-hf-temp0.8-p0-k0-bf16-n200-batch10-maxlen1024-java-generations-0-15_multiple-java_intermediate.json
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌| 299/300 [27:36<00:10, 10.69s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0-k0/CodeLlama-13b-Python-hf-temp0.8-p0-k0-bf16-n200-batch10-maxlen1024-java-generations-0-15_multiple-java_intermediate.json
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 300/300 [27:46<00:00,  5.56s/it]
generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0-k0/CodeLlama-13b-Python-hf-temp0.8-p0-k0-bf16-n200-batch10-maxlen1024-java-generations-0-15_multiple-java.json
references were saved at references_multiple-java.json
evaluation results:
{
  "config": {
    "prefix": "",
    "do_sample": true,
    "temperature": 0.8,
    "top_k": 0,
    "top_p": 0.0,
    "n_samples": 200,
    "eos": "<|endoftext|>",
    "seed": 0,
    "model": "codellama/CodeLlama-13b-Python-hf",
    "modeltype": "causal",
    "peft_model": null,
    "revision": null,
    "token": false,
    "trust_remote_code": true,
    "tasks": "multiple-java",
    "instruction_tokens": null,
    "batch_size": 10,
    "max_length_generation": 1024,
    "precision": "bf16",
    "load_in_8bit": false,
    "load_in_4bit": false,
    "limit": 15,
    "limit_start": 0,
    "save_every_k_tasks": 20,
    "postprocess": true,
    "allow_code_execution": true,
    "generation_only": true,
    "load_generations_path": null,
    "load_data_path": null,
    "metric_output_path": "evaluation_results.json",
    "save_generations": true,
    "load_generations_intermediate_paths": null,
    "save_generations_path": "./runpod/codellama-13b-python/java/improve/t0.8-p0-k0/CodeLlama-13b-Python-hf-temp0.8-p0-k0-bf16-n200-batch10-maxlen1024-java-generations-0-15.json",
    "save_references": true,
    "save_references_path": "references.json",
    "prompt": "prompt",
    "max_memory_per_gpu": null,
    "check_references": false
  }
}