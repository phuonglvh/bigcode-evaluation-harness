AUTHOR="codellama"
MODEL_NAME="CodeLlama-13b-Python-hf"

max_length=1024
temperature=0.8
top_k=0
top_p=0.95
batch_size=25

BASE_DIR=./runpod/$MODEL_NAME/$lang/improve/t$temperature-p$top_p-k$top_k-batch$batch_size
mkdir -p $BASE_DIR

n_samples=200
seed=0
precision=bf16
lang=java

# limit_start=18
# limit=3
limit_start=25
limit=25
eval_limit_start=0
eval_limit=50

save_every_k_tasks=1 # after completing k dataset's tasks
save_every_k_iterations=$((save_every_k_tasks * n_samples / batch_size))

common_name="$MODEL_NAME-temp$temperature-p$top_p-k$top_k-$precision-n$n_samples-batch$batch_size-maxlen$max_length-$lang"
generations_name="$common_name-generations-${limit_start}-${limit}_multiple-$lang"
generations_path="$BASE_DIR/$generations_name.json"

python main.py --model "$AUTHOR/$MODEL_NAME" \
    --tasks multiple-$lang \
    --max_memory_per_gpu autot \SE_DIR/$common_name-generations-${limit_start}-${limit}.json" \
Selected Tasks: ['multiple-java']
Loading model in bf16
Loading model in auto mode
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████| 3/3 [00:14<00:00,  4.75s/it]
generation only mode
number of problems for this task is 25
task range: 26->50
200 completions required for each task
25 completion/prompt
8 batch/task
200 batches (iterations) required for 25 tasks
  4%|██▊                                                                              | 7/200 [06:08<2:59:50, 55.91s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0-batch25/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch25-maxlen1024-java-generations-25-25_multiple-java_intermediate.json
  8%|██████                                                                          | 15/200 [12:45<2:35:02, 50.29s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0-batch25/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch25-maxlen1024-java-generations-25-25_multiple-java_intermediate.json
 12%|█████████▏                                                                      | 23/200 [15:58<1:04:36, 21.90s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0-batch25/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch25-maxlen1024-java-generations-25-25_multiple-java_intermediate.json
 16%|████████████▋                                                                     | 31/200 [18:44<55:09, 19.58s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0-batch25/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch25-maxlen1024-java-generations-25-25_multiple-java_intermediate.json
 20%|███████████████▌                                                                | 39/200 [23:26<1:37:28, 36.33s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0-batch25/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch25-maxlen1024-java-generations-25-25_multiple-java_intermediate.json
 24%|██████████████████▊                                                             | 47/200 [28:13<1:51:23, 43.68s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0-batch25/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch25-maxlen1024-java-generations-25-25_multiple-java_intermediate.json
 28%|██████████████████████                                                          | 55/200 [32:52<1:25:51, 35.53s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0-batch25/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch25-maxlen1024-java-generations-25-25_multiple-java_intermediate.json
 32%|█████████████████████████▏                                                      | 63/200 [36:54<1:42:56, 45.09s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0-batch25/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch25-maxlen1024-java-generations-25-25_multiple-java_intermediate.json
 36%|████████████████████████████▍                                                   | 71/200 [41:12<1:17:35, 36.09s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0-batch25/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch25-maxlen1024-java-generations-25-25_multiple-java_intermediate.json
 40%|████████████████████████████████▍                                                 | 79/200 [43:02<35:14, 17.48s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0-batch25/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch25-maxlen1024-java-generations-25-25_multiple-java_intermediate.json
 44%|██████████████████████████████████▊                                             | 87/200 [49:28<1:35:01, 50.46s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0-batch25/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch25-maxlen1024-java-generations-25-25_multiple-java_intermediate.json
 48%|██████████████████████████████████████▉                                           | 95/200 [52:01<30:33, 17.46s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0-batch25/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch25-maxlen1024-java-generations-25-25_multiple-java_intermediate.json
 52%|████████████████████████████████████████▋                                      | 103/200 [57:34<1:14:08, 45.86s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0-batch25/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch25-maxlen1024-java-generations-25-25_multiple-java_intermediate.json
 56%|██████████████████████████████████████████▋                                  | 111/200 [1:03:15<1:38:32, 66.43s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0-batch25/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch25-maxlen1024-java-generations-25-25_multiple-java_intermediate.json
 60%|█████████████████████████████████████████████▊                               | 119/200 [1:09:52<1:10:57, 52.57s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0-batch25/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch25-maxlen1024-java-generations-25-25_multiple-java_intermediate.json