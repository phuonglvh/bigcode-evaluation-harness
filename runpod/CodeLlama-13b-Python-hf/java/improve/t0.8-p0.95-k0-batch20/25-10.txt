root@C.11775336:/workspace/bigcode-evaluation-harness$ AUTHOR="codellama"
MODEL_NAME="CodeLlama-13b-Python-hf"

max_length=1024
temperature=0.8
top_p=0.95
top_k=0
batch_size=20

BASE_DIR=./runpod/$MODEL_NAME/$lang/improve/t$temperature-p$top_p-k$top_k-batch$batch_size
mkdir -p $BASE_DIR

n_samples=200
seed=0
precision=bf16
lang=java

limit_start=25
limit=25
eval_limit_start=0
eval_limit=50

save_every_k_tasks=1 # after completing k dataset's tasks
save_every_k_iterations=$((save_every_k_tasks * n_samples / batch_size))

common_name="$MODEL_NAME-temp$temperature-p$top_p-k$top_k-$precision-n$n_samples-batch$batch_size-maxlen$max_length-$lang"
generations_name="$common_name-generations-${limit_start}-${limit}_multiple-$lang"
generations_path="$BASE_DIR/$generations_name.json"

python main.py --model "$AUTHOR/$MODEL_NAME" \
    --tasks multiple-$lang \
    --max_length_generation $max_length \
    --temperature $temperature \
    --max_memory_per_gpu autot \SE_DIR/$common_name-generations-${limit_start}-${limit}.json" \
Selected Tasks: ['multiple-java']
Loading model in bf16
Loading model in auto mode
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████| 3/3 [00:10<00:00,  3.54s/it]
generation only mode
number of problems for this task is 25
task range: 26->50
200 completions required for each task
20 completion/prompt
10 batch/task
250 batches (iterations) required for 25 tasks
  4%|███▏                                                                                   | 9/250 [05:04<2:00:48, 30.08s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0-batch20/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch20-maxlen1024-java-generations-25-25_multiple-java_intermediate.json
  8%|██████▌                                                                               | 19/250 [10:13<2:02:42, 31.87s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0-batch20/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch20-maxlen1024-java-generations-25-25_multiple-java_intermediate.json
 12%|█████████▉                                                                            | 29/250 [15:25<3:11:54, 52.10s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0-batch20/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch20-maxlen1024-java-generations-25-25_multiple-java_intermediate.json
 16%|█████████████▋                                                                          | 39/250 [17:30<45:44, 13.01s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0-batch20/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch20-maxlen1024-java-generations-25-25_multiple-java_intermediate.json
 20%|████████████████▊                                                                     | 49/250 [22:10<1:45:50, 31.59s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0-batch20/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch20-maxlen1024-java-generations-25-25_multiple-java_intermediate.json
 24%|████████████████████▎                                                                 | 59/250 [26:24<1:19:46, 25.06s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0-batch20/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch20-maxlen1024-java-generations-25-25_multiple-java_intermediate.json
 28%|███████████████████████▋                                                              | 69/250 [30:48<1:18:53, 26.15s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0-batch20/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch20-maxlen1024-java-generations-25-25_multiple-java_intermediate.json
 32%|███████████████████████████▊                                                            | 79/250 [34:30<46:18, 16.25s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0-batch20/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch20-maxlen1024-java-generations-25-25_multiple-java_intermediate.json
 36%|███████████████████████████████▎                                                        | 89/250 [36:58<25:49,  9.62s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0-batch20/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch20-maxlen1024-java-generations-25-25_multiple-java_intermediate.json
 40%|██████████████████████████████████▊                                                     | 99/250 [38:45<24:59,  9.93s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0-batch20/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch20-maxlen1024-java-generations-25-25_multiple-java_intermediate.json
 40%|██████████████████████████████████▊                                                    | 100/250 [39:06<58:39, 23.46s/it]
Traceback (most recent call last):
  File "/workspace/bigcode-evaluation-harness/main.py", line 431, in <module>
    main()
  File "/workspace/bigcode-evaluation-harness/main.py", line 399, in main
    generations, references = evaluator.generate_text(
  File "/workspace/bigcode-evaluation-harness/bigcode_eval/evaluator.py", line 69, in generate_text
    generations = parallel_generations(
  File "/workspace/bigcode-evaluation-harness/bigcode_eval/generation.py", line 155, in parallel_generations
    generations = complete_code(
  File "/workspace/bigcode-evaluation-harness/bigcode_eval/utils.py", line 310, in complete_code
    generated_tokens = model.generate(
  File "/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py", line 1989, in generate
    result = self._sample(
  File "/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py", line 2932, in _sample
    outputs = self(**model_inputs, return_dict=True)
  File "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py", line 169, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1161, in forward
    logits = logits.float()
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.73 GiB. GPU 1 has a total capacity of 23.69 GiB of which 1.43 GiB is free. Process 1758913 has 22.24 GiB memory in use. Of the allocated memory 18.71 GiB is allocated by PyTorch, and 3.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)