{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/phuonglvh/projects/2170558-thesis-automatic-code-generation-using-machine-learning/bigcode-evaluation-harness/runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0-batch20\n",
      "15-10-stdout.txt\n",
      "25-10-hw.txt\n",
      "25-10.txt\n",
      "35-5-stdout.txt\n",
      "40-5-stdout.txt\n",
      "45-5-stdout.txt\n",
      "50-108-gen.txt\n",
      "CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch20-maxlen1024-java-generations-0-14_multiple-java.json\n",
      "CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch20-maxlen1024-java-generations-0-1_multiple-java.json\n",
      "CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch20-maxlen1024-java-generations-0-50_multiple-java-eval-0-50-evaluation_results.json\n",
      "CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch20-maxlen1024-java-generations-0-50_multiple-java.json\n",
      "CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch20-maxlen1024-java-generations-14-1_multiple-java.json\n",
      "CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch20-maxlen1024-java-generations-15-10_multiple-java.json\n",
      "CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch20-maxlen1024-java-generations-25-10_multiple-java.json\n",
      "CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch20-maxlen1024-java-generations-35-5_multiple-java.json\n",
      "CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch20-maxlen1024-java-generations-40-5_multiple-java.json\n",
      "CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch20-maxlen1024-java-generations-45-5_multiple-java.json\n",
      "CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch20-maxlen1024-java-generations-50-108_multiple-java.json\n",
      "eval.txt\n",
      "gen-eval-java.sh\n",
      "main.ipynb\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch20-maxlen1024-java-generations-0-14_multiple-java.json\n",
      "./CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch20-maxlen1024-java-generations-14-1_multiple-java.json\n",
      "./CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch20-maxlen1024-java-generations-15-10_multiple-java.json\n",
      "./CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch20-maxlen1024-java-generations-25-10_multiple-java.json\n",
      "./CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch20-maxlen1024-java-generations-35-5_multiple-java.json\n",
      "./CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch20-maxlen1024-java-generations-40-5_multiple-java.json\n",
      "./CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch20-maxlen1024-java-generations-45-5_multiple-java.json\n",
      "./CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch20-maxlen1024-java-generations-50-108_multiple-java.json\n"
     ]
    }
   ],
   "source": [
    "t = 0.8\n",
    "p = 0.95\n",
    "k = 0\n",
    "b = 20\n",
    "\n",
    "parts = [(0, 14), (14, 1), (15, 10), (25, 10),\n",
    "         (35, 5), (40, 5), (45, 5), (50, 108)]\n",
    "part_paths = [\n",
    "    f'./CodeLlama-13b-Python-hf-temp{t}-p{p}-k{k}-bf16-n200-batch{b}-maxlen1024-java-generations-{limit_start}-{limit}_multiple-java.json' for limit_start, limit in parts]\n",
    "\n",
    "print('\\n'.join(part_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "part_gens = [json.load(open(part_path, 'r')) for part_path in part_paths]\n",
    "\n",
    "all_gens = []\n",
    "for part_gen in part_gens:\n",
    "    all_gens.extend(part_gen)\n",
    "    \n",
    "print(len(all_gens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved 158 at \"./CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch20-maxlen1024-java-generations-0-158_multiple-java.json\"\n"
     ]
    }
   ],
   "source": [
    "start = 0\n",
    "(limit_start, limit) = parts[-1]\n",
    "stop = limit_start + limit\n",
    "\n",
    "merged_path = f'./CodeLlama-13b-Python-hf-temp{t}-p{p}-k{k}-bf16-n200-batch{b}-maxlen1024-java-generations-{start}-{stop}_multiple-java.json'\n",
    "json.dump(all_gens, open(merged_path, 'w'))\n",
    "print(f'saved {len(all_gens)} at \"{merged_path}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audit generations length done\n"
     ]
    }
   ],
   "source": [
    "all_gens = json.load(open(merged_path, 'r'))\n",
    "for task_id, gens in enumerate(all_gens):\n",
    "    if len(gens) != 200:\n",
    "        print(f'task_id {task_id} has length {len(gens)} instead of 200')\n",
    "\n",
    "print('audit generations length done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigcode-evaluation-harness",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
