python main.py --model "$AUTHOR/$MODEL_NAME"     --tasks multiple-$lang     --max_length_generation $max_length     --temperature $temperature     --top_p $top_p     --top_k $top_k     --seed $seed     --n_samples $n_samples     --batch_size $batch_size     --precision $precision     --allow_code_execution     --trust_remote_code     --save_every_k_tasks $save_every_k_iterations     --save_generations     --save_generations_path "$BASE_DIR/$common_name-generations-${limit_start}-${limit}.json"     --save_references     --generation_only     --limit_start $limit_start     --limit $limit     --max_memory_per_gpu auto
Selected Tasks: ['multiple-java']
Loading model in bf16
Loading model in auto mode
Loading checkpoint shards:   0%|                                                                  | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███████████████████▎                                      | 1/3 [00:01<00:03,  1.83s/it]Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████| 3/3 [00:04<00:00,  1.60s/it]
generation only mode
number of problems for this task is 25
task range: 1->25
200 completions required for each task
10 completion/prompt
20 batch/task
500 batches (iterations) required for 25 tasks
  4%|███                                                                               | 19/500 [00:42<15:54,  1.98s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.94-k0/CodeLlama-13b-Python-hf-temp0.8-p0.94-k0-bf16-n200-batch10-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
  8%|██████▏                                                                         | 39/500 [06:56<1:53:18, 14.75s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.94-k0/CodeLlama-13b-Python-hf-temp0.8-p0.94-k0-bf16-n200-batch10-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
 12%|█████████▍                                                                      | 59/500 [11:17<1:23:56, 11.42s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.94-k0/CodeLlama-13b-Python-hf-temp0.8-p0.94-k0-bf16-n200-batch10-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
 16%|████████████▉                                                                     | 79/500 [13:17<39:05,  5.57s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.94-k0/CodeLlama-13b-Python-hf-temp0.8-p0.94-k0-bf16-n200-batch10-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
 20%|███████████████▊                                                                | 99/500 [17:27<1:37:36, 14.60s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.94-k0/CodeLlama-13b-Python-hf-temp0.8-p0.94-k0-bf16-n200-batch10-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
 24%|██████████████████▊                                                            | 119/500 [21:11<1:13:25, 11.56s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.94-k0/CodeLlama-13b-Python-hf-temp0.8-p0.94-k0-bf16-n200-batch10-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
 28%|██████████████████████▌                                                          | 139/500 [24:17<49:22,  8.21s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.94-k0/CodeLlama-13b-Python-hf-temp0.8-p0.94-k0-bf16-n200-batch10-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
 32%|█████████████████████████▊                                                       | 159/500 [26:47<41:25,  7.29s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.94-k0/CodeLlama-13b-Python-hf-temp0.8-p0.94-k0-bf16-n200-batch10-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
 36%|████████████████████████████▎                                                  | 179/500 [29:53<1:06:56, 12.51s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.94-k0/CodeLlama-13b-Python-hf-temp0.8-p0.94-k0-bf16-n200-batch10-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
 40%|███████████████████████████████▍                                               | 199/500 [34:17<1:06:39, 13.29s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.94-k0/CodeLlama-13b-Python-hf-temp0.8-p0.94-k0-bf16-n200-batch10-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
 44%|███████████████████████████████████▍                                             | 219/500 [36:21<23:19,  4.98s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.94-k0/CodeLlama-13b-Python-hf-temp0.8-p0.94-k0-bf16-n200-batch10-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
 48%|██████████████████████████████████████▋                                          | 239/500 [37:45<17:31,  4.03s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.94-k0/CodeLlama-13b-Python-hf-temp0.8-p0.94-k0-bf16-n200-batch10-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
 52%|█████████████████████████████████████████▉                                       | 259/500 [38:16<06:18,  1.57s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.94-k0/CodeLlama-13b-Python-hf-temp0.8-p0.94-k0-bf16-n200-batch10-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
 56%|█████████████████████████████████████████████▏                                   | 279/500 [41:41<41:34, 11.29s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.94-k0/CodeLlama-13b-Python-hf-temp0.8-p0.94-k0-bf16-n200-batch10-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
 60%|███████████████████████████████████████████████▏                               | 299/500 [50:13<1:29:39, 26.76s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.94-k0/CodeLlama-13b-Python-hf-temp0.8-p0.94-k0-bf16-n200-batch10-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
 64%|█████████████████████████████████████████████████▏                           | 319/500 [1:00:21<1:41:39, 33.70s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.94-k0/CodeLlama-13b-Python-hf-temp0.8-p0.94-k0-bf16-n200-batch10-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
 68%|█████████████████████████████████████████████████████▌                         | 339/500 [1:02:55<17:35,  6.56s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.94-k0/CodeLlama-13b-Python-hf-temp0.8-p0.94-k0-bf16-n200-batch10-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
 72%|████████████████████████████████████████████████████████▋                      | 359/500 [1:11:25<59:38, 25.38s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.94-k0/CodeLlama-13b-Python-hf-temp0.8-p0.94-k0-bf16-n200-batch10-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
 76%|███████████████████████████████████████████████████████████▉                   | 379/500 [1:15:59<24:58, 12.38s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.94-k0/CodeLlama-13b-Python-hf-temp0.8-p0.94-k0-bf16-n200-batch10-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
 80%|███████████████████████████████████████████████████████████████                | 399/500 [1:20:00<20:21, 12.09s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.94-k0/CodeLlama-13b-Python-hf-temp0.8-p0.94-k0-bf16-n200-batch10-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
 84%|██████████████████████████████████████████████████████████████████▏            | 419/500 [1:23:52<23:01, 17.06s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.94-k0/CodeLlama-13b-Python-hf-temp0.8-p0.94-k0-bf16-n200-batch10-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
 88%|█████████████████████████████████████████████████████████████████████▎         | 439/500 [1:28:59<14:27, 14.22s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.94-k0/CodeLlama-13b-Python-hf-temp0.8-p0.94-k0-bf16-n200-batch10-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
 92%|████████████████████████████████████████████████████████████████████████▌      | 459/500 [1:30:51<03:49,  5.61s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.94-k0/CodeLlama-13b-Python-hf-temp0.8-p0.94-k0-bf16-n200-batch10-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
 96%|███████████████████████████████████████████████████████████████████████████▋   | 479/500 [1:34:41<04:43, 13.48s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.94-k0/CodeLlama-13b-Python-hf-temp0.8-p0.94-k0-bf16-n200-batch10-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
100%|██████████████████████████████████████████████████████████████████████████████▊| 499/500 [1:40:41<00:20, 20.82s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.94-k0/CodeLlama-13b-Python-hf-temp0.8-p0.94-k0-bf16-n200-batch10-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
100%|███████████████████████████████████████████████████████████████████████████████| 500/500 [1:40:53<00:00, 12.11s/it]
audit_generations: verifying generations against dataset
audit_generations: unknown_tasks []
generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.94-k0/CodeLlama-13b-Python-hf-temp0.8-p0.94-k0-bf16-n200-batch10-maxlen1024-java-generations-0-25_multiple-java.json
references were saved at references_multiple-java.json
evaluation results:
{
  "config": {
    "prefix": "",
    "do_sample": true,
    "temperature": 0.8,
    "top_k": 0,
    "top_p": 0.94,
    "n_samples": 200,
    "eos": "<|endoftext|>",
    "seed": 0,
    "model": "codellama/CodeLlama-13b-Python-hf",
    "modeltype": "causal",
    "peft_model": null,
    "revision": null,
    "token": false,
    "trust_remote_code": true,
    "tasks": "multiple-java",
    "instruction_tokens": null,
    "batch_size": 10,
    "max_length_generation": 1024,
    "precision": "bf16",
    "load_in_8bit": false,
    "load_in_4bit": false,
    "left_padding": false,
    "limit": 25,
    "limit_start": 0,
    "save_every_k_tasks": 20,
    "postprocess": true,
    "allow_code_execution": true,
    "generation_only": true,
    "load_generations_path": null,
    "load_data_path": null,
    "metric_output_path": "evaluation_results.json",
    "save_generations": true,
    "load_generations_intermediate_paths": null,
    "save_generations_path": "./runpod/codellama-13b-python/java/improve/t0.8-p0.94-k0/CodeLlama-13b-Python-hf-temp0.8-p0.94-k0-bf16-n200-batch10-maxlen1024-java-generations-0-25.json",
    "save_references": true,
    "save_references_path": "references.json",
    "prompt": "prompt",
    "max_memory_per_gpu": "auto",
    "check_references": false
  }
}