root@C.11778773:/workspace/bigcode-evaluation-harness$ AUTHOR="codellama"
MODEL_NAME="CodeLlama-13b-Python-hf"

max_length=1024
temperature=0.8
top_k=0
top_p=0.95
batch_size=40

BASE_DIR=./runpod/$MODEL_NAME/$lang/improve/t$temperature-p$top_p-k$top_k-batch$batch_size
mkdir -p $BASE_DIR

n_samples=200
seed=0
precision=bf16
lang=java

limit_start=10
limit=15
eval_limit_start=0
eval_limit=50

save_every_k_tasks=1 # after completing k dataset's tasks
save_every_k_iterations=$((save_every_k_tasks * n_samples / batch_size))

common_name="$MODEL_NAME-temp$temperature-p$top_p-k$top_k-$precision-n$n_samples-batch$batch_size-maxlen$max_length-$lang"
generations_name="$common_name-generations-${limit_start}-${limit}_multiple-$lang"
generations_path="$BASE_DIR/$generations_name.json"

python main.py --model "$AUTHOR/$MODEL_NAME" \
    --tasks multiple-$lang \
    --max_length_generation $max_length \
    --temperature $temperature \
    --max_memory_per_gpu autot \SE_DIR/$common_name-generations-${limit_start}-${limit}.json" \
Selected Tasks: ['multiple-java']
Loading model in bf16
Loading model in auto mode
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████| 3/3 [00:06<00:00,  2.14s/it]
generation only mode
number of problems for this task is 15
task range: 11->25
200 completions required for each task
40 completion/prompt
5 batch/task
75 batches (iterations) required for 15 tasks
  5%|████▋                                                                                   | 4/75 [04:23<1:44:18, 88.14s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0-batch40/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch40-maxlen1024-java-generations-10-15_multiple-java_intermediate.json
 12%|██████████▊                                                                               | 9/75 [05:27<24:00, 21.83s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0-batch40/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch40-maxlen1024-java-generations-10-15_multiple-java_intermediate.json
 19%|████████████████▌                                                                        | 14/75 [05:48<06:42,  6.60s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0-batch40/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch40-maxlen1024-java-generations-10-15_multiple-java_intermediate.json
 25%|██████████████████████▌                                                                  | 19/75 [08:57<35:03, 37.56s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0-batch40/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch40-maxlen1024-java-generations-10-15_multiple-java_intermediate.json
 27%|███████████████████████▋                                                                 | 20/75 [09:38<26:32, 28.95s/it]
Traceback (most recent call last):
  File "/workspace/bigcode-evaluation-harness/main.py", line 431, in <module>
    main()
  File "/workspace/bigcode-evaluation-harness/main.py", line 399, in main
    generations, references = evaluator.generate_text(
  File "/workspace/bigcode-evaluation-harness/bigcode_eval/evaluator.py", line 69, in generate_text
    generations = parallel_generations(
  File "/workspace/bigcode-evaluation-harness/bigcode_eval/generation.py", line 155, in parallel_generations
    generations = complete_code(
  File "/workspace/bigcode-evaluation-harness/bigcode_eval/utils.py", line 310, in complete_code
    generated_tokens = model.generate(
  File "/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py", line 1989, in generate
    result = self._sample(
  File "/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py", line 2932, in _sample
    outputs = self(**model_inputs, return_dict=True)
  File "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py", line 169, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1161, in forward
    logits = logits.float()
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.40 GiB. GPU 2 has a total capacity of 23.69 GiB of which 3.11 GiB is free. Process 453018 has 20.57 GiB memory in use. Of the allocated memory 17.06 GiB is allocated by PyTorch, and 3.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)