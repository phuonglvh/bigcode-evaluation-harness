AUTHOR="codellama"
MODEL_NAME="CodeLlama-13b-Python-hf"

max_length=1024
temperature=0.8
top_p=0.95
top_k=0

BASE_DIR=./runpod/$MODEL_NAME/$lang/improve/t$temperature-p$top_p-k$top_k
mkdir -p $BASE_DIR

n_samples=200
seed=0
precision=bf16
lang=java
batch_size=1

limit_start=0
limit=25
eval_limit_start=0
eval_limit=50

save_every_k_tasks=1 # after completing k dataset's tasks
save_every_k_iterations=$((save_every_k_tasks * n_samples / batch_size))

common_name="$MODEL_NAME-temp$temperature-p$top_p-k$top_k-$precision-n$n_samples-batch$batch_size-maxlen$max_length-$lang"
generations_name="$common_name-generations-${limit_start}-${limit}_multiple-$lang"
generations_path="$BASE_DIR/$generations_name.json"

python main.py --model "$AUTHOR/$MODEL_NAME" \
    --tasks multiple-$lang \
    --max_length_generation $max_length \
    --temperature $temperature \
    --max_memory_per_gpu autot \SE_DIR/$common_name-generations-${limit_start}-${limit}.json" \
Selected Tasks: ['multiple-java']
Loading model in bf16
Loading model in auto mode
config.json: 100%|█████████████████████████████████████████████████████████████████████| 589/589 [00:00<00:00, 2.63MB/s]
model.safetensors.index.json: 100%|████████████████████████████████████████████████| 31.4k/31.4k [00:00<00:00, 55.4MB/s]
model-00001-of-00003.safetensors: 100%|████████████████████████████████████████████| 9.95G/9.95G [03:29<00:00, 47.6MB/s]
model-00002-of-00003.safetensors: 100%|████████████████████████████████████████████| 9.90G/9.90G [03:22<00:00, 48.8MB/s]
model-00003-of-00003.safetensors: 100%|████████████████████████████████████████████| 6.18G/6.18G [02:16<00:00, 45.4MB/s]
Downloading shards: 100%|████████████████████████████████████████████████████████████████| 3/3 [09:09<00:00, 183.06s/it]
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████| 3/3 [00:05<00:00,  1.99s/it]
generation_config.json: 100%|███████████████████████████████████████████████████████████| 116/116 [00:00<00:00, 679kB/s]
tokenizer_config.json: 100%|███████████████████████████████████████████████████████████| 749/749 [00:00<00:00, 4.03MB/s]
tokenizer.model: 100%|███████████████████████████████████████████████████████████████| 500k/500k [00:00<00:00, 9.14MB/s]
tokenizer.json: 100%|██████████████████████████████████████████████████████████████| 1.84M/1.84M [00:00<00:00, 25.6MB/s]
special_tokens_map.json: 100%|█████████████████████████████████████████████████████████| 411/411 [00:00<00:00, 2.37MB/s]
generation only mode
Downloading builder script: 100%|██████████████████████████████████████████████████| 4.05k/4.05k [00:00<00:00, 14.6MB/s]
Downloading metadata: 100%|██████████████████████████████████████████████████████████| 478k/478k [00:00<00:00, 16.1MB/s]
Downloading readme: 100%|██████████████████████████████████████████████████████████| 99.6k/99.6k [00:00<00:00, 18.1MB/s]
Downloading data: 321kB [00:00, 75.6MB/s]                                                                               
Generating test split: 100%|█████████████████████████████████████████████████| 158/158 [00:00<00:00, 6661.31 examples/s]
number of problems for this task is 25
task range: 1->25
200 completions required for each task
1 completion/prompt
200 batch/task
5000 batches (iterations) required for 25 tasks
  4%|███                                                                           | 199/5000 [03:32<1:15:39,  1.06it/s]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch1-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
  8%|██████▏                                                                      | 399/5000 [42:01<10:31:05,  8.23s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch1-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
 12%|█████████                                                                   | 599/5000 [1:04:39<7:32:57,  6.18s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch1-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
 16%|████████████▏                                                               | 799/5000 [1:20:18<6:01:53,  5.17s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch1-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
 20%|███████████████▏                                                            | 999/5000 [1:45:39<6:46:17,  6.09s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch1-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
 24%|█████████████████▉                                                         | 1199/5000 [2:06:41<6:45:45,  6.40s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch1-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
 28%|████████████████████▉                                                      | 1399/5000 [2:23:16<5:11:42,  5.19s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch1-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
 32%|███████████████████████▉                                                   | 1599/5000 [2:38:11<4:31:56,  4.80s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch1-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
 36%|██████████████████████████▉                                                | 1799/5000 [2:54:34<8:05:55,  9.11s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch1-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
 40%|█████████████████████████████▉                                             | 1999/5000 [3:24:03<7:47:48,  9.35s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch1-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
 44%|████████████████████████████████▉                                          | 2199/5000 [3:35:03<1:57:50,  2.52s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch1-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
 48%|███████████████████████████████████▉                                       | 2399/5000 [3:48:25<3:00:34,  4.17s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch1-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
 52%|████████████████████████████████████████                                     | 2599/5000 [3:51:09<28:19,  1.41it/s]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch1-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
 56%|█████████████████████████████████████████▉                                 | 2799/5000 [4:08:30<3:19:21,  5.43s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch1-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
 60%|████████████████████████████████████████████▉                              | 2999/5000 [4:48:53<6:59:19, 12.57s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch1-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
 44%|████████████████████████████████▉                                          | 2199/5000 [3:35:03<1:57:50,  2.52s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch1-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
 48%|███████████████████████████████████▉                                       | 2399/5000 [3:48:25<3:00:34,  4.17s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch1-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
 52%|████████████████████████████████████████                                     | 2599/5000 [3:51:09<28:19,  1.41it/s]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch1-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
 56%|█████████████████████████████████████████▉                                 | 2799/5000 [4:08:30<3:19:21,  5.43s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch1-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
 60%|████████████████████████████████████████████▉                              | 2999/5000 [4:48:53<6:59:19, 12.57s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch1-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
 64%|███████████████████████████████████████████████▉                           | 3199/5000 [5:38:08<6:04:16, 12.14s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch1-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
 68%|██████████████████████████████████████████████████▉                        | 3399/5000 [5:57:58<2:19:41,  5.23s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch1-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
 72%|█████████████████████████████████████████████████████▉                     | 3599/5000 [6:44:49<4:57:06, 12.72s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch1-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
 76%|████████████████████████████████████████████████████████▉                  | 3799/5000 [7:07:39<2:06:24,  6.32s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch1-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
 80%|███████████████████████████████████████████████████████████▉               | 3999/5000 [7:30:30<1:35:01,  5.70s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch1-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
 84%|████████████████████████████████████████████████████████████████▋            | 4199/5000 [7:43:24<51:02,  3.82s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch1-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
 88%|█████████████████████████████████████████████████████████████████▉         | 4399/5000 [8:09:08<2:08:13, 12.80s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch1-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
 92%|██████████████████████████████████████████████████████████████████████▊      | 4599/5000 [8:20:07<18:37,  2.79s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch1-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
 96%|█████████████████████████████████████████████████████████████████████████▉   | 4799/5000 [8:40:26<36:40, 10.95s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch1-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
100%|████████████████████████████████████████████████████████████████████████████▉| 4999/5000 [9:12:46<00:08,  8.47s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch1-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
100%|█████████████████████████████████████████████████████████████████████████████| 5000/5000 [9:12:54<00:00,  6.63s/it]
audit_generations: verifying generations against dataset
audit_generations: unknown_tasks []
generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch1-maxlen1024-java-generations-0-25_multiple-java.json
references were saved at references_multiple-java.json
evaluation results:
{
  "config": {
    "prefix": "",
    "do_sample": true,
    "temperature": 0.8,
    "top_k": 0,
    "top_p": 0.95,
    "n_samples": 200,
    "eos": "<|endoftext|>",
    "seed": 0,
    "model": "codellama/CodeLlama-13b-Python-hf",
    "modeltype": "causal",
    "peft_model": null,
    "revision": null,
    "token": false,
    "trust_remote_code": true,
    "tasks": "multiple-java",
    "instruction_tokens": null,
    "batch_size": 1,
    "max_length_generation": 1024,
    "precision": "bf16",
    "load_in_8bit": false,
    "load_in_4bit": false,
    "left_padding": false,
    "limit": 25,
    "limit_start": 0,
    "save_every_k_tasks": 200,
    "postprocess": true,
    "allow_code_execution": true,
    "generation_only": true,
    "load_generations_path": null,
    "load_data_path": null,
    "metric_output_path": "evaluation_results.json",
    "save_generations": true,
    "load_generations_intermediate_paths": null,
    "save_generations_path": "./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch1-maxlen1024-java-generations-0-25.json",
    "save_references": true,
    "save_references_path": "references.json",
    "prompt": "prompt",
    "max_memory_per_gpu": "auto",
    "check_references": false
  }
}