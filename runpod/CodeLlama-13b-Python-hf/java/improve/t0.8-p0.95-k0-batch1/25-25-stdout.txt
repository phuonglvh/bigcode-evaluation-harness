AUTHOR="codellama"
MODEL_NAME="CodeLlama-13b-Python-hf"
max_length=1024

temperature=0.8
top_p=0.95
top_k=0

BASE_DIR=./runpod/$MODEL_NAME/$lang/improve/t$temperature-p$top_p-k$top_k
mkdir -p $BASE_DIR

n_samples=200
seed=0
precision=bf16
lang=java
batch_size=1

limit_start=25
limit=25
eval_limit_start=0
eval_limit=50

save_every_k_tasks=1 # after completing k dataset's tasks
save_every_k_iterations=$((save_every_k_tasks * n_samples / batch_size))

common_name="$MODEL_NAME-temp$temperature-p$top_p-k$top_k-$precision-n$n_samples-batch$batch_size-maxlen$max_length-$lang"
generations_name="$common_name-generations-${limit_start}-${limit}_multiple-$lang"
generations_path="$BASE_DIR/$generations_name.json"

python main.py --model "$AUTHOR/$MODEL_NAME" \
    --tasks multiple-$lang \
    --max_length_generation $max_length \
    --temperature $temperature \
    --max_memory_per_gpu autot \SE_DIR/$common_name-generations-${limit_start}-${limit}.json" \
Selected Tasks: ['multiple-java']
Loading model in bf16
Loading model in auto mode
config.json: 100%|███████████████████████████████████████████████████████████████████████████| 589/589 [00:00<00:00, 2.64MB/s]
model.safetensors.index.json: 100%|██████████████████████████████████████████████████████| 31.4k/31.4k [00:00<00:00, 55.7MB/s]
model-00001-of-00003.safetensors: 100%|██████████████████████████████████████████████████| 9.95G/9.95G [01:56<00:00, 85.2MB/s]
model-00002-of-00003.safetensors: 100%|██████████████████████████████████████████████████| 9.90G/9.90G [02:01<00:00, 81.7MB/s]
model-00003-of-00003.safetensors: 100%|██████████████████████████████████████████████████| 6.18G/6.18G [01:08<00:00, 90.8MB/s]
Downloading shards: 100%|██████████████████████████████████████████████████████████████████████| 3/3 [05:06<00:00, 102.24s/it]
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████| 3/3 [00:30<00:00, 10.09s/it]
generation_config.json: 100%|█████████████████████████████████████████████████████████████████| 116/116 [00:00<00:00, 532kB/s]
tokenizer_config.json: 100%|█████████████████████████████████████████████████████████████████| 749/749 [00:00<00:00, 3.61MB/s]
tokenizer.model: 100%|██████████████████████████████████████████████████████████████████████| 500k/500k [00:00<00:00, 147MB/s]
tokenizer.json: 100%|████████████████████████████████████████████████████████████████████| 1.84M/1.84M [00:00<00:00, 3.79MB/s]
special_tokens_map.json: 100%|███████████████████████████████████████████████████████████████| 411/411 [00:00<00:00, 2.16MB/s]
generation only mode
Downloading builder script: 100%|████████████████████████████████████████████████████████| 4.05k/4.05k [00:00<00:00, 16.2MB/s]
Downloading metadata: 100%|████████████████████████████████████████████████████████████████| 478k/478k [00:00<00:00, 1.62MB/s]
Downloading readme: 100%|█████████████████████████████████████████████████████████████████| 99.6k/99.6k [00:00<00:00, 939kB/s]
Downloading data: 321kB [00:00, 61.2MB/s]                                                                                     
Generating test split: 100%|███████████████████████████████████████████████████████| 158/158 [00:00<00:00, 3556.80 examples/s]
number of problems for this task is 25
task range: 26->50
200 completions required for each task
1 completion/prompt
200 batch/task
5000 batches (iterations) required for 25 tasks
  4%|███▎                                                                               | 199/5000 [29:36<20:08:33, 15.10s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch1-maxlen1024-java-generations-25-25_multiple-java_intermediate.json
  8%|██████▌                                                                            | 399/5000 [59:52<11:10:11,  8.74s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch1-maxlen1024-java-generations-25-25_multiple-java_intermediate.json
 12%|█████████▊                                                                        | 599/5000 [1:15:19<7:29:40,  6.13s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch1-maxlen1024-java-generations-25-25_multiple-java_intermediate.json
 16%|█████████████                                                                     | 799/5000 [1:32:44<5:49:06,  4.99s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch1-maxlen1024-java-generations-25-25_multiple-java_intermediate.json
 20%|████████████████▍                                                                 | 999/5000 [1:55:40<7:37:17,  6.86s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch1-maxlen1024-java-generations-25-25_multiple-java_intermediate.json
 24%|███████████████████▏                                                            | 1199/5000 [2:32:38<12:30:25, 11.85s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch1-maxlen1024-java-generations-25-25_multiple-java_intermediate.json
 28%|██████████████████████▍                                                         | 1399/5000 [3:07:26<10:24:09, 10.40s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch1-maxlen1024-java-generations-25-25_multiple-java_intermediate.json
 32%|█████████████████████████▉                                                       | 1599/5000 [3:27:02<5:00:08,  5.30s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch1-maxlen1024-java-generations-25-25_multiple-java_intermediate.json
 36%|█████████████████████████████▏                                                   | 1799/5000 [3:38:24<2:40:16,  3.00s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch1-maxlen1024-java-generations-25-25_multiple-java_intermediate.json
 40%|████████████████████████████████▍                                                | 1999/5000 [3:49:26<2:27:34,  2.95s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch1-maxlen1024-java-generations-25-25_multiple-java_intermediate.json
 44%|███████████████████████████████████▌                                             | 2199/5000 [4:28:43<5:06:00,  6.56s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch1-maxlen1024-java-generations-25-25_multiple-java_intermediate.json
 48%|██████████████████████████████████████▊                                          | 2399/5000 [4:49:12<4:23:37,  6.08s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch1-maxlen1024-java-generations-25-25_multiple-java_intermediate.json
 52%|██████████████████████████████████████████                                       | 2599/5000 [5:09:31<4:53:21,  7.33s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch1-maxlen1024-java-generations-25-25_multiple-java_intermediate.json
 56%|█████████████████████████████████████████████▎                                   | 2799/5000 [5:24:11<2:25:22,  3.96s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch1-maxlen1024-java-generations-25-25_multiple-java_intermediate.json
 60%|████████████████████████████████████████████████▌                                | 2999/5000 [5:57:09<4:36:42,  8.30s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch1-maxlen1024-java-generations-25-25_multiple-java_intermediate.json
 64%|███████████████████████████████████████████████████▊                             | 3199/5000 [6:20:56<2:41:35,  5.38s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch1-maxlen1024-java-generations-25-25_multiple-java_intermediate.json
 68%|███████████████████████████████████████████████████████                          | 3399/5000 [6:31:01<1:10:00,  2.62s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch1-maxlen1024-java-generations-25-25_multiple-java_intermediate.json
 72%|██████████████████████████████████████████████████████████▎                      | 3599/5000 [7:12:40<5:02:06, 12.94s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch1-maxlen1024-java-generations-25-25_multiple-java_intermediate.json
 76%|█████████████████████████████████████████████████████████████▌                   | 3799/5000 [7:33:01<1:31:13,  4.56s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch1-maxlen1024-java-generations-25-25_multiple-java_intermediate.json
 80%|████████████████████████████████████████████████████████████████▊                | 3999/5000 [7:57:00<3:09:42, 11.37s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch1-maxlen1024-java-generations-25-25_multiple-java_intermediate.json
 84%|████████████████████████████████████████████████████████████████████             | 4199/5000 [8:16:50<1:26:35,  6.49s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch1-maxlen1024-java-generations-25-25_multiple-java_intermediate.json
 88%|███████████████████████████████████████████████████████████████████████▎         | 4399/5000 [9:24:31<3:20:56, 20.06s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch1-maxlen1024-java-generations-25-25_multiple-java_intermediate.json
 92%|████████████████████████████████████████████████████████████████████████████▎      | 4599/5000 [9:41:06<30:14,  4.53s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch1-maxlen1024-java-generations-25-25_multiple-java_intermediate.json
 96%|███████████████████████████████████████████████████████████████████████████████▋   | 4799/5000 [9:52:51<11:39,  3.48s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch1-maxlen1024-java-generations-25-25_multiple-java_intermediate.json
100%|█████████████████████████████████████████████████████████████████████████████████▉| 4999/5000 [10:09:57<00:05,  5.82s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch1-maxlen1024-java-generations-25-25_multiple-java_intermediate.json
100%|██████████████████████████████████████████████████████████████████████████████████| 5000/5000 [10:10:03<00:00,  7.32s/it]
audit_generations: verifying generations against dataset
audit_generations: unknown_tasks [(0, 'public static boolean moveOneBall(ArrayList<Long> arr) {}'), (1, 'public static Pair<Long, Long> evenOddPalindrome(long n) {}'), (2, 'public static boolean isEqualToSumEven(long n) {}'), (3, 'public static ArrayList<Long> derivative(ArrayList<Long> xs) {}'), (4, 'public static boolean isSorted(ArrayList<Long> lst) {}'), (5, 'public static String solve(String s) {}'), (6, 'public static ArrayList<Long> tri(long n) {}'), (7, 'public static long fizzBuzz(long n) {}'), (8, 'public static ArrayList<String> filterByPrefix(ArrayList<String> strings, String prefix) {}'), (9, 'public static String solve(long N) {}'), (10, 'public static ArrayList<Long> minPath(ArrayList<ArrayList<Long>> grid, long k) {}'), (11, 'public static long countUpper(String s) {}'), (12, 'public static ArrayList<Long> maximum(ArrayList<Long> arr, long k) {}'), (13, 'public static long largestDivisor(long n) {}'), (14, 'public static ArrayList<Long> sortArray(ArrayList<Long> array) {}'), (15, 'public static ArrayList<Long> f(long n) {}'), (16, 'public static boolean iscube(long a) {}'), (17, 'public static String encode(String message) {}'), (18, 'public static long isBored(String S) {}'), (19, 'public static boolean pairsSumToZero(ArrayList<Long> l) {}'), (20, 'public static float triangleArea(long a, long b, long c) {}'), (21, 'public static ArrayList<String> bf(String planet1, String planet2) {}'), (22, 'public static long digits(long n) {}'), (23, 'public static ArrayList<String> wordsString(String s) {}'), (24, 'public static long howManyTimes(String string, String substring) {}')]
generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch1-maxlen1024-java-generations-25-25_multiple-java.json
references were saved at references_multiple-java.json
evaluation results:
{
  "config": {
    "prefix": "",
    "do_sample": true,
    "temperature": 0.8,
    "top_k": 0,
    "top_p": 0.95,
    "n_samples": 200,
    "eos": "<|endoftext|>",
    "seed": 0,
    "model": "codellama/CodeLlama-13b-Python-hf",
    "modeltype": "causal",
    "peft_model": null,
    "revision": null,
    "token": false,
    "trust_remote_code": true,
    "tasks": "multiple-java",
    "instruction_tokens": null,
    "batch_size": 1,
    "max_length_generation": 1024,
    "precision": "bf16",
    "load_in_8bit": false,
    "load_in_4bit": false,
    "left_padding": false,
    "limit": 25,
    "limit_start": 25,
    "save_every_k_tasks": 200,
    "postprocess": true,
    "allow_code_execution": true,
    "generation_only": true,
    "load_generations_path": null,
    "load_data_path": null,
    "metric_output_path": "evaluation_results.json",
    "save_generations": true,
    "load_generations_intermediate_paths": null,
    "save_generations_path": "./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch1-maxlen1024-java-generations-25-25.json",
    "save_references": true,
    "save_references_path": "references.json",
    "prompt": "prompt",
    "max_memory_per_gpu": "auto",
    "check_references": false
  }
}