root@C.11874045:/workspace/bigcode-evaluation-harness$ AUTHOR="codellama"
MODEL_NAME="CodeLlama-13b-Python-hf"
max_length=1024

temperature=0.8
top_k=0
top_p=0.95
batch_size=60

BASE_DIR=./runpod/$MODEL_NAME/$lang/improve/t$temperature-p$top_p-k$top_k-batch$batch_size
mkdir -p $BASE_DIR

n_samples=200
seed=0
precision=bf16
lang=java

limit_start=0
limit=25
eval_limit_start=0
eval_limit=158

save_every_k_tasks=1 # after completing k dataset's tasks
save_every_k_iterations=$((save_every_k_tasks * n_samples / batch_size))

common_name="$MODEL_NAME-temp$temperature-p$top_p-k$top_k-$precision-n$n_samples-batch$batch_size-maxlen$max_length-$lang"
generations_name="$common_name-generations-${limit_start}-${limit}_multiple-$lang"
generations_path="$BASE_DIR/$generations_name.json"

python main.py --model "$AUTHOR/$MODEL_NAME" \
    --tasks multiple-$lang \
    --max_length_generation $max_length \
    --temperature $temperature \
    --max_memory_per_gpu autot \SE_DIR/$common_name-generations-${limit_start}-${limit}.json" \
Selected Tasks: ['multiple-java']
Loading model in bf16
Loading model in auto mode
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████| 3/3 [00:07<00:00,  2.66s/it]
generation only mode
number of problems for this task is 25
task range: 1->25
200 completions required for each task
60 completion/prompt
4 batch/task
100 batches (iterations) required for 25 tasks
  2%|█▊                                                                                       | 2/100 [00:05<04:05,  2.50s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0-batch60/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch60-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
  5%|████▍                                                                                    | 5/100 [01:20<42:31, 26.86s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0-batch60/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch60-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
  8%|██████▉                                                                                | 8/100 [05:02<1:32:28, 60.31s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0-batch60/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch60-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
 11%|█████████▋                                                                              | 11/100 [06:15<52:21, 35.30s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0-batch60/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch60-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
 14%|████████████▎                                                                           | 14/100 [06:49<27:14, 19.00s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0-batch60/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch60-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
 17%|██████████████▉                                                                         | 17/100 [07:30<23:01, 16.64s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0-batch60/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch60-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
 20%|█████████████████▌                                                                      | 20/100 [08:29<25:10, 18.88s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0-batch60/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch60-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
 23%|████████████████████▏                                                                   | 23/100 [09:11<19:59, 15.57s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0-batch60/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch60-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
 26%|██████████████████████▉                                                                 | 26/100 [09:46<16:05, 13.05s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0-batch60/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch60-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
 29%|█████████████████████████▌                                                              | 29/100 [11:40<28:47, 24.33s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0-batch60/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch60-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
 32%|████████████████████████████▏                                                           | 32/100 [13:44<45:53, 40.49s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0-batch60/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch60-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
 35%|██████████████████████████████                                                        | 35/100 [17:06<1:05:43, 60.67s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0-batch60/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch60-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
 38%|█████████████████████████████████▍                                                      | 38/100 [19:07<53:32, 51.82s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0-batch60/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch60-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
 41%|████████████████████████████████████                                                    | 41/100 [19:58<27:17, 27.76s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0-batch60/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch60-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
 44%|██████████████████████████████████████▋                                                 | 44/100 [20:28<14:36, 15.66s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0-batch60/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch60-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
 47%|█████████████████████████████████████████▎                                              | 47/100 [20:41<07:15,  8.22s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0-batch60/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch60-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
 50%|████████████████████████████████████████████                                            | 50/100 [20:46<03:12,  3.85s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0-batch60/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch60-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
 53%|██████████████████████████████████████████████▋                                         | 53/100 [21:04<04:44,  6.05s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.8-p0.95-k0-batch60/CodeLlama-13b-Python-hf-temp0.8-p0.95-k0-bf16-n200-batch60-maxlen1024-java-generations-0-25_multiple-java_intermediate.json
 56%|█████████████████████████████████████████████████▎                                      | 56/100 [21:58<17:15, 23.54s/it]
Traceback (most recent call last):
  File "/workspace/bigcode-evaluation-harness/main.py", line 431, in <module>
    main()
  File "/workspace/bigcode-evaluation-harness/main.py", line 399, in main
    generations, references = evaluator.generate_text(
  File "/workspace/bigcode-evaluation-harness/bigcode_eval/evaluator.py", line 69, in generate_text
    generations = parallel_generations(
  File "/workspace/bigcode-evaluation-harness/bigcode_eval/generation.py", line 155, in parallel_generations
    generations = complete_code(
  File "/workspace/bigcode-evaluation-harness/bigcode_eval/utils.py", line 310, in complete_code
    generated_tokens = model.generate(
  File "/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py", line 2024, in generate
    result = self._sample(
  File "/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py", line 2982, in _sample
    outputs = self(**model_inputs, return_dict=True)
  File "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1209, in forward
    logits = logits.float()
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 5.09 GiB. GPU 0 has a total capacity of 79.11 GiB of which 2.52 GiB is free. Process 1438197 has 76.57 GiB memory in use. Of the allocated memory 59.94 GiB is allocated by PyTorch, and 15.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
root@C.11874045:/workspace/bigcode-evaluation-harness$ 
root@C.11874045:/workspace/bigcode-evaluation-harness$ AUTHOR="codellama"
MODEL_NAME="CodeLlama-13b-Python-hf"
max_length=1024

temperature=0.8
top_k=0
top_p=0.95
batch_size=60

BASE_DIR=./runpod/$MODEL_NAME/$lang/improve/t$temperature-p$top_p-k$top_k-batch$batch_size
mkdir -p $BASE_DIR

n_samples=200
seed=0
precision=bf16
lang=java

limit_start=14
limit=11
eval_limit_start=0
eval_limit=158

save_every_k_tasks=1 # after completing k dataset's tasks
save_every_k_iterations=$((save_every_k_tasks * n_samples / batch_size))

common_name="$MODEL_NAME-temp$temperature-p$top_p-k$top_k-$precision-n$n_samples-batch$batch_size-maxlen$max_length-$lang"
generations_name="$common_name-generations-${limit_start}-${limit}_multiple-$lang"
generations_path="$BASE_DIR/$generations_name.json"
root@C.11874045:/workspace/bigcode-evaluation-harness$ AUTHOR="codellama"
MODEL_NAME="CodeLlama-13b-Python-hf"
max_length=1024

temperature=0.8
top_k=0
top_p=0.95
batch_size=60

BASE_DIR=./runpod/$MODEL_NAME/$lang/improve/t$temperature-p$top_p-k$top_k-batch$batch_size
mkdir -p $BASE_DIR

n_samples=200
seed=0
precision=bf16
lang=java

limit_start=14
limit=11
eval_limit_start=0
eval_limit=158

save_every_k_tasks=1 # after completing k dataset's tasks
save_every_k_iterations=$((save_every_k_tasks * n_samples / batch_size))

common_name="$MODEL_NAME-temp$temperature-p$top_p-k$top_k-$precision-n$n_samples-batch$batch_size-maxlen$max_length-$lang"
generations_name="$common_name-generations-${limit_start}-${limit}_multiple-$lang"
generations_path="$BASE_DIR/$generations_name.json"

python main.py --model "$AUTHOR/$MODEL_NAME" \
    --tasks multiple-$lang \
    --max_length_generation $max_length \
    --temperature $temperature \
    --max_memory_per_gpu autot \SE_DIR/$common_name-generations-${limit_start}-${limit}.json" \
Selected Tasks: ['multiple-java']
Loading model in bf16
Loading model in auto mode
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████| 3/3 [00:06<00:00,  2.16s/it]
generation only mode
number of problems for this task is 11
task range: 15->25
200 completions required for each task
60 completion/prompt
4 batch/task
44 batches (iterations) required for 11 tasks
  0%|                                                                                                  | 0/44 [00:02<?, ?it/s]
Traceback (most recent call last):
  File "/workspace/bigcode-evaluation-harness/main.py", line 431, in <module>
    main()
  File "/workspace/bigcode-evaluation-harness/main.py", line 399, in main
    generations, references = evaluator.generate_text(
  File "/workspace/bigcode-evaluation-harness/bigcode_eval/evaluator.py", line 69, in generate_text
    generations = parallel_generations(
  File "/workspace/bigcode-evaluation-harness/bigcode_eval/generation.py", line 155, in parallel_generations
    generations = complete_code(
  File "/workspace/bigcode-evaluation-harness/bigcode_eval/utils.py", line 310, in complete_code
    generated_tokens = model.generate(
  File "/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py", line 2024, in generate
    result = self._sample(
  File "/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py", line 2982, in _sample
    outputs = self(**model_inputs, return_dict=True)
  File "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1189, in forward
    outputs = self.model(
  File "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1001, in forward
    layer_outputs = decoder_layer(
  File "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 734, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
  File "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 640, in forward
    key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)
  File "/opt/conda/lib/python3.10/site-packages/transformers/cache_utils.py", line 383, in update
    self.key_cache[layer_idx] = torch.cat([self.key_cache[layer_idx], key_states], dim=-2)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 418.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 272.50 MiB is free. Process 1476979 has 78.83 GiB memory in use. Of the allocated memory 57.04 GiB is allocated by PyTorch, and 20.95 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)