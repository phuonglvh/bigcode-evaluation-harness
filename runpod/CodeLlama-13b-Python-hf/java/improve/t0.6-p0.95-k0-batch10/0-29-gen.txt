root@C.13098415:/workspace/bigcode-evaluation-harness$ AUTHOR="codellama"
MODEL_NAME="CodeLlama-13b-Python-hf"
max_length=1024

temperature=0.6
top_k=0
top_p=0.95
batch_size=10

BASE_DIR=./runpod/$MODEL_NAME/$lang/improve/t$temperature-p$top_p-k$top_k-batch$batch_size
mkdir -p $BASE_DIR

n_samples=200
seed=0
precision=bf16
lang=java

limit_start=0
limit=158
eval_limit_start=0
eval_limit=158

save_every_k_tasks=1 # after completing k dataset's tasks
save_every_k_iterations=$((save_every_k_tasks * n_samples / batch_size))

common_name="$MODEL_NAME-temp$temperature-p$top_p-k$top_k-$precision-n$n_samples-batch$batch_size-maxlen$max_length-$lang"
generations_name="$common_name-generations-${limit_start}-${limit}_multiple-$lang"
generations_path="$BASE_DIR/$generations_name.json"

python main.py --model "$AUTHOR/$MODEL_NAME" \
    --tasks multiple-$lang \
    --max_length_generation $max_length \
    --temperature $temperature \
    --max_memory_per_gpu autot \SE_DIR/$common_name-generations-${limit_start}-${limit}.json" \
Selected Tasks: ['multiple-java']
Loading model in bf16
Loading model in auto mode
config.json: 100%|███████████████████████████████████████████████████████████████████████████| 589/589 [00:00<00:00, 4.33MB/s]
model.safetensors.index.json: 100%|██████████████████████████████████████████████████████| 31.4k/31.4k [00:00<00:00, 43.3MB/s]
model-00001-of-00003.safetensors: 100%|██████████████████████████████████████████████████| 9.95G/9.95G [01:53<00:00, 87.3MB/s]
model-00002-of-00003.safetensors: 100%|██████████████████████████████████████████████████| 9.90G/9.90G [03:38<00:00, 45.2MB/s]
model-00003-of-00003.safetensors: 100%|██████████████████████████████████████████████████| 6.18G/6.18G [01:08<00:00, 89.6MB/s]
Downloading shards: 100%|██████████████████████████████████████████████████████████████████████| 3/3 [06:42<00:00, 134.16s/it]
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████| 3/3 [00:05<00:00,  1.90s/it]
generation_config.json: 100%|████████████████████████████████████████████████████████████████| 116/116 [00:00<00:00, 2.12MB/s]
tokenizer_config.json: 100%|█████████████████████████████████████████████████████████████████| 749/749 [00:00<00:00, 8.42MB/s]
tokenizer.model: 100%|██████████████████████████████████████████████████████████████████████| 500k/500k [00:00<00:00, 210MB/s]
tokenizer.json: 100%|████████████████████████████████████████████████████████████████████| 1.84M/1.84M [00:00<00:00, 5.68MB/s]
special_tokens_map.json: 100%|███████████████████████████████████████████████████████████████| 411/411 [00:00<00:00, 3.42MB/s]
generation only mode
Downloading builder script: 100%|████████████████████████████████████████████████████████| 4.05k/4.05k [00:00<00:00, 40.6kB/s]
Downloading metadata: 100%|████████████████████████████████████████████████████████████████| 478k/478k [00:00<00:00, 4.44MB/s]
Downloading readme: 100%|█████████████████████████████████████████████████████████████████| 99.6k/99.6k [00:00<00:00, 658kB/s]
Downloading data: 321kB [00:00, 80.7MB/s]                                                                                     
Generating test split: 100%|███████████████████████████████████████████████████████| 158/158 [00:00<00:00, 4669.50 examples/s]
number of problems for this task is 158
task range: 1->158
200 completions required for each task
10 completion/prompt
20 batch/task
3160 batches (iterations) required for 158 tasks
  1%|▌                                                                                    | 19/3160 [00:31<1:23:06,  1.59s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.6-p0.95-k0-batch10/CodeLlama-13b-Python-hf-temp0.6-p0.95-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
  1%|█                                                                                   | 39/3160 [10:36<27:17:10, 31.47s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.6-p0.95-k0-batch10/CodeLlama-13b-Python-hf-temp0.6-p0.95-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
  2%|█▌                                                                                  | 59/3160 [15:29<11:24:30, 13.24s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.6-p0.95-k0-batch10/CodeLlama-13b-Python-hf-temp0.6-p0.95-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
  2%|██▏                                                                                  | 79/3160 [17:39<5:28:47,  6.40s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.6-p0.95-k0-batch10/CodeLlama-13b-Python-hf-temp0.6-p0.95-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
  3%|██▋                                                                                 | 99/3160 [22:02<11:07:14, 13.08s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.6-p0.95-k0-batch10/CodeLlama-13b-Python-hf-temp0.6-p0.95-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
  4%|███▏                                                                               | 119/3160 [26:22<10:29:42, 12.42s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.6-p0.95-k0-batch10/CodeLlama-13b-Python-hf-temp0.6-p0.95-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
  4%|███▋                                                                                | 139/3160 [30:04<6:27:16,  7.69s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.6-p0.95-k0-batch10/CodeLlama-13b-Python-hf-temp0.6-p0.95-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
  5%|████▏                                                                               | 159/3160 [33:00<8:04:32,  9.69s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.6-p0.95-k0-batch10/CodeLlama-13b-Python-hf-temp0.6-p0.95-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
  6%|████▊                                                                               | 179/3160 [36:22<7:30:34,  9.07s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.6-p0.95-k0-batch10/CodeLlama-13b-Python-hf-temp0.6-p0.95-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
  6%|█████▏                                                                             | 199/3160 [41:20<13:00:57, 15.82s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.6-p0.95-k0-batch10/CodeLlama-13b-Python-hf-temp0.6-p0.95-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
  7%|█████▊                                                                              | 219/3160 [43:25<5:02:05,  6.16s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.6-p0.95-k0-batch10/CodeLlama-13b-Python-hf-temp0.6-p0.95-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
  8%|██████▎                                                                             | 239/3160 [44:57<3:48:39,  4.70s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.6-p0.95-k0-batch10/CodeLlama-13b-Python-hf-temp0.6-p0.95-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
  8%|██████▉                                                                             | 259/3160 [45:26<1:05:40,  1.36s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.6-p0.95-k0-batch10/CodeLlama-13b-Python-hf-temp0.6-p0.95-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
  9%|███████▍                                                                            | 279/3160 [48:55<9:14:40, 11.55s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.6-p0.95-k0-batch10/CodeLlama-13b-Python-hf-temp0.6-p0.95-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
  9%|███████▋                                                                         | 299/3160 [1:00:56<29:16:45, 36.84s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.6-p0.95-k0-batch10/CodeLlama-13b-Python-hf-temp0.6-p0.95-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
 10%|████████▏                                                                        | 319/3160 [1:11:34<21:22:34, 27.09s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.6-p0.95-k0-batch10/CodeLlama-13b-Python-hf-temp0.6-p0.95-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
 11%|████████▊                                                                         | 339/3160 [1:14:34<5:57:12,  7.60s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.6-p0.95-k0-batch10/CodeLlama-13b-Python-hf-temp0.6-p0.95-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
 11%|█████████▏                                                                       | 359/3160 [1:24:52<25:19:56, 32.56s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.6-p0.95-k0-batch10/CodeLlama-13b-Python-hf-temp0.6-p0.95-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
 12%|█████████▊                                                                        | 379/3160 [1:29:59<9:31:08, 12.32s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.6-p0.95-k0-batch10/CodeLlama-13b-Python-hf-temp0.6-p0.95-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
 13%|██████████▏                                                                      | 399/3160 [1:36:29<19:05:21, 24.89s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.6-p0.95-k0-batch10/CodeLlama-13b-Python-hf-temp0.6-p0.95-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
 13%|██████████▊                                                                       | 419/3160 [1:39:15<5:59:23,  7.87s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.6-p0.95-k0-batch10/CodeLlama-13b-Python-hf-temp0.6-p0.95-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
 14%|███████████▎                                                                     | 439/3160 [1:45:20<16:14:36, 21.49s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.6-p0.95-k0-batch10/CodeLlama-13b-Python-hf-temp0.6-p0.95-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
 15%|███████████▉                                                                      | 459/3160 [1:47:21<4:11:27,  5.59s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.6-p0.95-k0-batch10/CodeLlama-13b-Python-hf-temp0.6-p0.95-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
 15%|████████████▍                                                                     | 479/3160 [1:50:58<7:47:59, 10.47s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.6-p0.95-k0-batch10/CodeLlama-13b-Python-hf-temp0.6-p0.95-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
 16%|████████████▉                                                                     | 499/3160 [1:56:49<9:49:57, 13.30s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.6-p0.95-k0-batch10/CodeLlama-13b-Python-hf-temp0.6-p0.95-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
 16%|█████████████▎                                                                   | 519/3160 [2:04:32<16:22:44, 22.33s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.6-p0.95-k0-batch10/CodeLlama-13b-Python-hf-temp0.6-p0.95-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
 17%|█████████████▊                                                                   | 539/3160 [2:11:21<15:20:23, 21.07s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.6-p0.95-k0-batch10/CodeLlama-13b-Python-hf-temp0.6-p0.95-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
 18%|██████████████▌                                                                   | 559/3160 [2:15:53<7:03:47,  9.78s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.6-p0.95-k0-batch10/CodeLlama-13b-Python-hf-temp0.6-p0.95-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
 18%|███████████████                                                                   | 579/3160 [2:19:32<5:53:08,  8.21s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.6-p0.95-k0-batch10/CodeLlama-13b-Python-hf-temp0.6-p0.95-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
 19%|███████████████▎                                                                 | 599/3160 [2:26:47<11:44:58, 16.52s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.6-p0.95-k0-batch10/CodeLlama-13b-Python-hf-temp0.6-p0.95-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
 19%|███████████████▊                                                                 | 616/3160 [2:31:33<11:24:23, 16.14s/it]