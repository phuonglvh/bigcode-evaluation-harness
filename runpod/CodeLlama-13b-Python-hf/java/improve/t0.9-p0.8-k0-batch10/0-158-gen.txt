root@C.13098466:/workspace/bigcode-evaluation-harness$ AUTHOR="codellama"
MODEL_NAME="CodeLlama-13b-Python-hf"

max_length=1024
temperature=0.9
top_k=0
top_p=0.8
batch_size=10

BASE_DIR=./runpod/$MODEL_NAME/$lang/improve/t$temperature-p$top_p-k$top_k-batch$batch_size
mkdir -p $BASE_DIR

n_samples=200
seed=0
precision=bf16
lang=java

limit_start=0
limit=158
eval_limit_start=0
eval_limit=158

save_every_k_tasks=1 # after completing k dataset's tasks
save_every_k_iterations=$((save_every_k_tasks * n_samples / batch_size))

common_name="$MODEL_NAME-temp$temperature-p$top_p-k$top_k-$precision-n$n_samples-batch$batch_size-maxlen$max_length-$lang"
generations_name="$common_name-generations-${limit_start}-${limit}_multiple-$lang"
generations_path="$BASE_DIR/$generations_name.json"
intermediate_generations_path="$BASE_DIR/$generations_name" 
intermediate_generations_path+="_intermediate.json"

echo $generations_path
    --max_memory_per_gpu autot \ate_paths "$intermediate_generations_path" \t}-${limit}.json" \
./runpod/codellama-13b-python/java/improve/t0.9-p0.8-k0-batch10/CodeLlama-13b-Python-hf-temp0.9-p0.8-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java.json
./runpod/codellama-13b-python/java/improve/t0.9-p0.8-k0-batch10/CodeLlama-13b-Python-hf-temp0.9-p0.8-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
Selected Tasks: ['multiple-java']
Loading model in bf16
Loading model in auto mode
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████| 3/3 [00:11<00:00,  3.97s/it]
task multiple-java: loading intermediate generations from ['./runpod/codellama-13b-python/java/improve/t0.9-p0.8-k0-batch10/CodeLlama-13b-Python-hf-temp0.9-p0.8-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json']
task multiple-java: loaded 158 intermediate generations from ['./runpod/codellama-13b-python/java/improve/t0.9-p0.8-k0-batch10/CodeLlama-13b-Python-hf-temp0.9-p0.8-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json']
generation only mode
number of problems for this task is 71
task range: 88->158
200 completions required for each task
10 completion/prompt
20 batch/task
1420 batches (iterations) required for 71 tasks
  1%|█                                                                             | 19/1420 [08:19<10:11:26, 26.19s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.9-p0.8-k0-batch10/CodeLlama-13b-Python-hf-temp0.9-p0.8-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
  3%|██▏                                                                            | 39/1420 [16:08<7:12:59, 18.81s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.9-p0.8-k0-batch10/CodeLlama-13b-Python-hf-temp0.9-p0.8-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
  4%|███▏                                                                          | 59/1420 [24:56<11:27:26, 30.31s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.9-p0.8-k0-batch10/CodeLlama-13b-Python-hf-temp0.9-p0.8-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
  6%|████▍                                                                          | 79/1420 [31:23<6:50:37, 18.37s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.9-p0.8-k0-batch10/CodeLlama-13b-Python-hf-temp0.9-p0.8-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
root@C.13098466:/workspace/bigcode-evaluation-harness/runpod/codellama-13b-python/java/improve/t0.9-p0.8-k0-batch10$ cd /workspace/bigcode-evaluation-harness/
root@C.13098466:/workspace/bigcode-evaluation-harness$ AUTHOR="codellama"
MODEL_NAME="CodeLlama-13b-Python-hf"

max_length=1024
temperature=0.9
top_k=0
top_p=0.8
batch_size=10

BASE_DIR=./runpod/$MODEL_NAME/$lang/improve/t$temperature-p$top_p-k$top_k-batch$batch_size
mkdir -p $BASE_DIR

n_samples=200
seed=0
precision=bf16
lang=java

limit_start=0
limit=158
eval_limit_start=0
eval_limit=158

save_every_k_tasks=1 # after completing k dataset's tasks
save_every_k_iterations=$((save_every_k_tasks * n_samples / batch_size))

common_name="$MODEL_NAME-temp$temperature-p$top_p-k$top_k-$precision-n$n_samples-batch$batch_size-maxlen$max_length-$lang"
generations_name="$common_name-generations-${limit_start}-${limit}_multiple-$lang"
generations_path="$BASE_DIR/$generations_name.json"
intermediate_generations_path="$BASE_DIR/$generations_name" 
intermediate_generations_path+="_intermediate.json"

echo $generations_path
    --max_memory_per_gpu autot \ate_paths "$intermediate_generations_path" \t}-${limit}.json" \
./runpod/codellama-13b-python/java/improve/t0.9-p0.8-k0-batch10/CodeLlama-13b-Python-hf-temp0.9-p0.8-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java.json
./runpod/codellama-13b-python/java/improve/t0.9-p0.8-k0-batch10/CodeLlama-13b-Python-hf-temp0.9-p0.8-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
Selected Tasks: ['multiple-java']
Loading model in bf16
Loading model in auto mode
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████| 3/3 [00:11<00:00,  3.97s/it]
task multiple-java: loading intermediate generations from ['./runpod/codellama-13b-python/java/improve/t0.9-p0.8-k0-batch10/CodeLlama-13b-Python-hf-temp0.9-p0.8-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json']
task multiple-java: loaded 158 intermediate generations from ['./runpod/codellama-13b-python/java/improve/t0.9-p0.8-k0-batch10/CodeLlama-13b-Python-hf-temp0.9-p0.8-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json']
generation only mode
number of problems for this task is 71
task range: 88->158
200 completions required for each task
10 completion/prompt
20 batch/task
1420 batches (iterations) required for 71 tasks
  1%|█                                                                             | 19/1420 [08:19<10:11:26, 26.19s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.9-p0.8-k0-batch10/CodeLlama-13b-Python-hf-temp0.9-p0.8-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
  3%|██▏                                                                            | 39/1420 [16:08<7:12:59, 18.81s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.9-p0.8-k0-batch10/CodeLlama-13b-Python-hf-temp0.9-p0.8-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
  4%|███▏                                                                          | 59/1420 [24:56<11:27:26, 30.31s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.9-p0.8-k0-batch10/CodeLlama-13b-Python-hf-temp0.9-p0.8-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
  6%|████▍                                                                          | 79/1420 [31:23<6:50:37, 18.37s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.9-p0.8-k0-batch10/CodeLlama-13b-Python-hf-temp0.9-p0.8-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
  7%|█████▌                                                                         | 99/1420 [34:35<3:25:09,  9.32s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.9-p0.8-k0-batch10/CodeLlama-13b-Python-hf-temp0.9-p0.8-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
  8%|██████▌                                                                       | 119/1420 [38:55<5:58:19, 16.53s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.9-p0.8-k0-batch10/CodeLlama-13b-Python-hf-temp0.9-p0.8-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
 10%|███████▋                                                                      | 139/1420 [42:05<3:09:13,  8.86s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.9-p0.8-k0-batch10/CodeLlama-13b-Python-hf-temp0.9-p0.8-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
 11%|████████▋                                                                     | 159/1420 [50:55<8:57:51, 25.59s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.9-p0.8-k0-batch10/CodeLlama-13b-Python-hf-temp0.9-p0.8-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
 13%|█████████▊                                                                    | 179/1420 [54:46<3:55:17, 11.38s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.9-p0.8-k0-batch10/CodeLlama-13b-Python-hf-temp0.9-p0.8-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
 14%|██████████▉                                                                   | 199/1420 [57:57<4:05:48, 12.08s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.9-p0.8-k0-batch10/CodeLlama-13b-Python-hf-temp0.9-p0.8-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
 15%|███████████▋                                                                | 219/1420 [1:00:25<2:16:20,  6.81s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.9-p0.8-k0-batch10/CodeLlama-13b-Python-hf-temp0.9-p0.8-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
 17%|████████████▊                                                               | 239/1420 [1:09:40<8:34:39, 26.15s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.9-p0.8-k0-batch10/CodeLlama-13b-Python-hf-temp0.9-p0.8-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
 18%|█████████████▊                                                              | 259/1420 [1:13:50<3:44:10, 11.59s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.9-p0.8-k0-batch10/CodeLlama-13b-Python-hf-temp0.9-p0.8-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
 20%|██████████████▉                                                             | 279/1420 [1:19:00<5:45:53, 18.19s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.9-p0.8-k0-batch10/CodeLlama-13b-Python-hf-temp0.9-p0.8-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
 21%|████████████████                                                            | 299/1420 [1:26:01<5:26:15, 17.46s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.9-p0.8-k0-batch10/CodeLlama-13b-Python-hf-temp0.9-p0.8-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
 22%|█████████████████                                                           | 319/1420 [1:31:52<4:25:07, 14.45s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.9-p0.8-k0-batch10/CodeLlama-13b-Python-hf-temp0.9-p0.8-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
 24%|██████████████████▏                                                         | 339/1420 [1:38:39<5:42:49, 19.03s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.9-p0.8-k0-batch10/CodeLlama-13b-Python-hf-temp0.9-p0.8-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
 25%|███████████████████▏                                                        | 359/1420 [1:47:39<7:24:36, 25.14s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.9-p0.8-k0-batch10/CodeLlama-13b-Python-hf-temp0.9-p0.8-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
 27%|████████████████████▎                                                       | 379/1420 [1:52:09<3:27:58, 11.99s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.9-p0.8-k0-batch10/CodeLlama-13b-Python-hf-temp0.9-p0.8-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
 28%|█████████████████████▎                                                      | 399/1420 [1:55:20<2:48:52,  9.92s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.9-p0.8-k0-batch10/CodeLlama-13b-Python-hf-temp0.9-p0.8-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
 30%|██████████████████████▍                                                     | 419/1420 [2:04:44<8:11:43, 29.47s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.9-p0.8-k0-batch10/CodeLlama-13b-Python-hf-temp0.9-p0.8-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
 31%|███████████████████████▍                                                    | 439/1420 [2:10:27<3:48:21, 13.97s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.9-p0.8-k0-batch10/CodeLlama-13b-Python-hf-temp0.9-p0.8-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
 32%|████████████████████████▌                                                   | 459/1420 [2:14:08<3:01:02, 11.30s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.9-p0.8-k0-batch10/CodeLlama-13b-Python-hf-temp0.9-p0.8-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
 34%|█████████████████████████▋                                                  | 479/1420 [2:20:59<5:26:03, 20.79s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.9-p0.8-k0-batch10/CodeLlama-13b-Python-hf-temp0.9-p0.8-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
 35%|██████████████████████████▋                                                 | 499/1420 [2:23:19<1:40:21,  6.54s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.9-p0.8-k0-batch10/CodeLlama-13b-Python-hf-temp0.9-p0.8-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
 37%|███████████████████████████▊                                                | 519/1420 [2:26:03<2:01:29,  8.09s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.9-p0.8-k0-batch10/CodeLlama-13b-Python-hf-temp0.9-p0.8-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
 38%|████████████████████████████▊                                               | 539/1420 [2:32:58<5:08:38, 21.02s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.9-p0.8-k0-batch10/CodeLlama-13b-Python-hf-temp0.9-p0.8-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
 39%|█████████████████████████████▉                                              | 559/1420 [2:35:21<1:34:08,  6.56s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.9-p0.8-k0-batch10/CodeLlama-13b-Python-hf-temp0.9-p0.8-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
 41%|██████████████████████████████▉                                             | 579/1420 [2:37:36<1:34:19,  6.73s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.9-p0.8-k0-batch10/CodeLlama-13b-Python-hf-temp0.9-p0.8-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
 42%|████████████████████████████████                                            | 599/1420 [2:42:34<2:59:22, 13.11s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.9-p0.8-k0-batch10/CodeLlama-13b-Python-hf-temp0.9-p0.8-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
 44%|█████████████████████████████████▏                                          | 619/1420 [2:51:06<8:42:49, 39.16s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.9-p0.8-k0-batch10/CodeLlama-13b-Python-hf-temp0.9-p0.8-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
 45%|███████████████████████████████████                                           | 639/1420 [2:53:30<53:06,  4.08s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.9-p0.8-k0-batch10/CodeLlama-13b-Python-hf-temp0.9-p0.8-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
 46%|███████████████████████████████████▎                                        | 659/1420 [2:58:09<2:30:49, 11.89s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.9-p0.8-k0-batch10/CodeLlama-13b-Python-hf-temp0.9-p0.8-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
 48%|████████████████████████████████████▎                                       | 679/1420 [3:02:48<2:34:31, 12.51s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.9-p0.8-k0-batch10/CodeLlama-13b-Python-hf-temp0.9-p0.8-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
 49%|█████████████████████████████████████▍                                      | 699/1420 [3:06:30<1:43:21,  8.60s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.9-p0.8-k0-batch10/CodeLlama-13b-Python-hf-temp0.9-p0.8-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
 51%|███████████████████████████████████████▍                                      | 719/1420 [3:07:46<42:23,  3.63s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.9-p0.8-k0-batch10/CodeLlama-13b-Python-hf-temp0.9-p0.8-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
 52%|███████████████████████████████████████▌                                    | 739/1420 [3:12:19<2:42:34, 14.32s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.9-p0.8-k0-batch10/CodeLlama-13b-Python-hf-temp0.9-p0.8-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
 53%|████████████████████████████████████████▌                                   | 759/1420 [3:14:25<1:01:27,  5.58s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.9-p0.8-k0-batch10/CodeLlama-13b-Python-hf-temp0.9-p0.8-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
 55%|█████████████████████████████████████████▋                                  | 779/1420 [3:21:41<3:31:28, 19.80s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.9-p0.8-k0-batch10/CodeLlama-13b-Python-hf-temp0.9-p0.8-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
 56%|██████████████████████████████████████████▊                                 | 799/1420 [3:26:39<3:22:26, 19.56s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.9-p0.8-k0-batch10/CodeLlama-13b-Python-hf-temp0.9-p0.8-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
 58%|████████████████████████████████████████████▉                                 | 819/1420 [3:27:39<15:23,  1.54s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.9-p0.8-k0-batch10/CodeLlama-13b-Python-hf-temp0.9-p0.8-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
 59%|████████████████████████████████████████████▉                               | 839/1420 [3:32:17<1:44:41, 10.81s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.9-p0.8-k0-batch10/CodeLlama-13b-Python-hf-temp0.9-p0.8-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
 60%|█████████████████████████████████████████████▉                              | 859/1420 [3:34:45<1:16:05,  8.14s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.9-p0.8-k0-batch10/CodeLlama-13b-Python-hf-temp0.9-p0.8-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
 62%|███████████████████████████████████████████████                             | 879/1420 [3:40:40<4:19:06, 28.74s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.9-p0.8-k0-batch10/CodeLlama-13b-Python-hf-temp0.9-p0.8-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
 63%|████████████████████████████████████████████████                            | 899/1420 [3:57:48<7:20:37, 50.74s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.9-p0.8-k0-batch10/CodeLlama-13b-Python-hf-temp0.9-p0.8-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
 65%|█████████████████████████████████████████████████▏                          | 919/1420 [4:02:46<1:23:40, 10.02s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.9-p0.8-k0-batch10/CodeLlama-13b-Python-hf-temp0.9-p0.8-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
 66%|██████████████████████████████████████████████████▎                         | 939/1420 [4:08:57<2:43:14, 20.36s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.9-p0.8-k0-batch10/CodeLlama-13b-Python-hf-temp0.9-p0.8-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
 68%|████████████████████████████████████████████████████▋                         | 959/1420 [4:11:03<48:43,  6.34s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.9-p0.8-k0-batch10/CodeLlama-13b-Python-hf-temp0.9-p0.8-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
 69%|████████████████████████████████████████████████████▍                       | 979/1420 [4:16:34<1:51:44, 15.20s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.9-p0.8-k0-batch10/CodeLlama-13b-Python-hf-temp0.9-p0.8-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
 70%|██████████████████████████████████████████████████████▊                       | 999/1420 [4:17:40<19:23,  2.76s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.9-p0.8-k0-batch10/CodeLlama-13b-Python-hf-temp0.9-p0.8-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
 72%|█████████████████████████████████████████████████████▊                     | 1019/1420 [4:22:01<1:17:51, 11.65s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.9-p0.8-k0-batch10/CodeLlama-13b-Python-hf-temp0.9-p0.8-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
 73%|████████████████████████████████████████████████████████▎                    | 1039/1420 [4:23:32<25:55,  4.08s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.9-p0.8-k0-batch10/CodeLlama-13b-Python-hf-temp0.9-p0.8-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
 75%|█████████████████████████████████████████████████████████▍                   | 1059/1420 [4:26:34<40:10,  6.68s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.9-p0.8-k0-batch10/CodeLlama-13b-Python-hf-temp0.9-p0.8-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
 76%|██████████████████████████████████████████████████████████▌                  | 1079/1420 [4:29:43<55:46,  9.81s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.9-p0.8-k0-batch10/CodeLlama-13b-Python-hf-temp0.9-p0.8-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
 77%|██████████████████████████████████████████████████████████                 | 1099/1420 [4:34:31<1:11:27, 13.36s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.9-p0.8-k0-batch10/CodeLlama-13b-Python-hf-temp0.9-p0.8-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
 79%|████████████████████████████████████████████████████████████▋                | 1119/1420 [4:39:38<53:12, 10.60s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.9-p0.8-k0-batch10/CodeLlama-13b-Python-hf-temp0.9-p0.8-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
 80%|████████████████████████████████████████████████████████████▏              | 1139/1420 [4:43:46<1:01:04, 13.04s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.9-p0.8-k0-batch10/CodeLlama-13b-Python-hf-temp0.9-p0.8-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
 82%|██████████████████████████████████████████████████████████████▊              | 1159/1420 [4:46:48<35:36,  8.19s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.9-p0.8-k0-batch10/CodeLlama-13b-Python-hf-temp0.9-p0.8-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
 83%|██████████████████████████████████████████████████████████████▎            | 1179/1420 [4:57:51<2:16:30, 33.99s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.9-p0.8-k0-batch10/CodeLlama-13b-Python-hf-temp0.9-p0.8-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
 84%|█████████████████████████████████████████████████████████████████            | 1199/1420 [5:01:15<32:08,  8.73s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.9-p0.8-k0-batch10/CodeLlama-13b-Python-hf-temp0.9-p0.8-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
 86%|██████████████████████████████████████████████████████████████████           | 1219/1420 [5:04:07<29:24,  8.78s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.9-p0.8-k0-batch10/CodeLlama-13b-Python-hf-temp0.9-p0.8-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
 87%|███████████████████████████████████████████████████████████████████▏         | 1239/1420 [5:05:23<10:01,  3.32s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.9-p0.8-k0-batch10/CodeLlama-13b-Python-hf-temp0.9-p0.8-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
 89%|████████████████████████████████████████████████████████████████████▎        | 1259/1420 [5:09:20<26:29,  9.87s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.9-p0.8-k0-batch10/CodeLlama-13b-Python-hf-temp0.9-p0.8-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
 90%|█████████████████████████████████████████████████████████████████████▎       | 1279/1420 [5:13:38<31:53, 13.57s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.9-p0.8-k0-batch10/CodeLlama-13b-Python-hf-temp0.9-p0.8-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
 91%|██████████████████████████████████████████████████████████████████████▍      | 1299/1420 [5:17:25<18:29,  9.17s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.9-p0.8-k0-batch10/CodeLlama-13b-Python-hf-temp0.9-p0.8-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
 93%|███████████████████████████████████████████████████████████████████████▌     | 1319/1420 [5:19:30<10:03,  5.97s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.9-p0.8-k0-batch10/CodeLlama-13b-Python-hf-temp0.9-p0.8-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
 94%|████████████████████████████████████████████████████████████████████████▌    | 1339/1420 [5:24:39<24:35, 18.22s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.9-p0.8-k0-batch10/CodeLlama-13b-Python-hf-temp0.9-p0.8-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
 96%|█████████████████████████████████████████████████████████████████████████▋   | 1359/1420 [5:30:42<17:00, 16.72s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.9-p0.8-k0-batch10/CodeLlama-13b-Python-hf-temp0.9-p0.8-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
 97%|██████████████████████████████████████████████████████████████████████████▊  | 1379/1420 [5:37:03<12:32, 18.36s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.9-p0.8-k0-batch10/CodeLlama-13b-Python-hf-temp0.9-p0.8-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
 99%|███████████████████████████████████████████████████████████████████████████▊ | 1399/1420 [5:42:35<05:45, 16.46s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.9-p0.8-k0-batch10/CodeLlama-13b-Python-hf-temp0.9-p0.8-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
100%|████████████████████████████████████████████████████████████████████████████▉| 1419/1420 [5:47:39<00:12, 12.42s/it]intermediate generations were saved at ./runpod/codellama-13b-python/java/improve/t0.9-p0.8-k0-batch10/CodeLlama-13b-Python-hf-temp0.9-p0.8-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json
100%|█████████████████████████████████████████████████████████████████████████████| 1420/1420 [5:47:47<00:00, 14.70s/it]
audit_generations: verifying generations against dataset
audit_generations: unknown_tasks []
generations were saved at ./runpod/codellama-13b-python/java/improve/t0.9-p0.8-k0-batch10/CodeLlama-13b-Python-hf-temp0.9-p0.8-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java.json
references were saved at references_multiple-java.json
evaluation results:
{
  "config": {
    "prefix": "",
    "do_sample": true,
    "temperature": 0.9,
    "top_k": 0,
    "top_p": 0.8,
    "n_samples": 200,
    "eos": "<|endoftext|>",
    "seed": 0,
    "model": "codellama/CodeLlama-13b-Python-hf",
    "modeltype": "causal",
    "peft_model": null,
    "revision": null,
    "token": false,
    "trust_remote_code": true,
    "tasks": "multiple-java",
    "instruction_tokens": null,
    "batch_size": 10,
    "max_length_generation": 1024,
    "precision": "bf16",
    "load_in_8bit": false,
    "load_in_4bit": false,
    "left_padding": false,
    "limit": 158,
    "limit_start": 0,
    "save_every_k_tasks": 20,
    "postprocess": true,
    "allow_code_execution": true,
    "generation_only": true,
    "load_generations_path": null,
    "load_data_path": null,
    "metric_output_path": "evaluation_results.json",
    "save_generations": true,
    "load_generations_intermediate_paths": [
      "./runpod/codellama-13b-python/java/improve/t0.9-p0.8-k0-batch10/CodeLlama-13b-Python-hf-temp0.9-p0.8-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158_multiple-java_intermediate.json"
    ],
    "save_generations_path": "./runpod/codellama-13b-python/java/improve/t0.9-p0.8-k0-batch10/CodeLlama-13b-Python-hf-temp0.9-p0.8-k0-bf16-n200-batch10-maxlen1024-java-generations-0-158.json",
    "save_references": true,
    "save_references_path": "references.json",
    "prompt": "prompt",
    "max_memory_per_gpu": "auto",
    "check_references": false
  }
}
root@C.13098466:/workspace/bigcode-evaluation-harness$ 