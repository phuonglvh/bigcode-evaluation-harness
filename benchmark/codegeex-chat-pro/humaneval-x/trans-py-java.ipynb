{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import dotenv_values\n",
    "\n",
    "env_path = os.path.join(os.getcwd(), '../../..', '.env')\n",
    "env_dict = dotenv_values(env_path)\n",
    "\n",
    "os.environ['CODEGEEX_CHAT_PRO_TOKEN'] = env_dict['CODEGEEX_CHAT_PRO_TOKEN']\n",
    "\n",
    "cwd = os.getcwd()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v1: 70.73"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(f'{cwd}/../../../')\n",
    "\n",
    "from bigcode_eval.tasks.code_to_code.codegeex_chat_pro import codegeex_chat_pro_translate_and_postprocess\n",
    "\n",
    "prompt_version='v1'\n",
    "\n",
    "translated_prompts_path = f'{cwd}/../../datasets/humaneval-x/humaneval_python_java_prompts_{prompt_version}.json'\n",
    "\n",
    "limit_start = 0\n",
    "limit = len(json.load(open(translated_prompts_path, \"r\")))\n",
    "model = 'codegeex-chat-pro'\n",
    "\n",
    "save_translated_codes_path = f'{cwd}/{model}-humaneval_python_java_prompts_{prompt_version}-translations-{limit_start}-{limit}.json'\n",
    "\n",
    "codegeex_chat_pro_translate_and_postprocess(\n",
    "    translated_prompts_path, save_translated_codes_path, limit_start, limit, parallel=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v1.1: 66.46"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using parallel processing with #5 workers\n",
      "Translated 164 prompts in parallel\n",
      "saved #164 translations to /Users/phuonglvh/projects/2170558-thesis-automatic-code-generation-using-machine-learning/bigcode-evaluation-harness/benchmark/codegeex-chat-pro/humaneval-x/codegeex-chat-pro-humaneval_python_java_prompts_v1.2-translations-0-164.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(f'{cwd}/../../../')\n",
    "\n",
    "from bigcode_eval.tasks.code_to_code.codegeex_chat_pro import codegeex_chat_pro_translate_and_postprocess\n",
    "\n",
    "prompt_version='v1.2'\n",
    "\n",
    "translated_prompts_path = f'{cwd}/../../datasets/humaneval-x/humaneval_python_java_prompts_{prompt_version}.json'\n",
    "\n",
    "limit_start = 0\n",
    "limit = len(json.load(open(translated_prompts_path, \"r\")))\n",
    "model = 'codegeex-chat-pro'\n",
    "\n",
    "save_translated_codes_path = f'{cwd}/{model}-humaneval_python_java_prompts_{prompt_version}-translations-{limit_start}-{limit}.json'\n",
    "\n",
    "codegeex_chat_pro_translate_and_postprocess(\n",
    "    translated_prompts_path, save_translated_codes_path, limit_start, limit, parallel=True, max_workers=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using parallel processing with #5 workers\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 19\u001b[0m\n\u001b[1;32m     15\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcodegeex-chat-pro\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     17\u001b[0m save_translated_codes_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcwd\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-humaneval_python_java_prompts_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt_version\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-translations-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlimit_start\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlimit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 19\u001b[0m \u001b[43mcodegeex_chat_pro_translate_and_postprocess\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtranslated_prompts_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_translated_codes_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/2170558-thesis-automatic-code-generation-using-machine-learning/bigcode-evaluation-harness/benchmark/codegeex-chat-pro/humaneval-x/../../../bigcode_eval/tasks/code_to_code/codegeex_chat_pro.py:95\u001b[0m, in \u001b[0;36mcodegeex_chat_pro_translate_and_postprocess\u001b[0;34m(translated_prompts_path, save_translations_path, limit_start, limit, parallel, max_workers)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;66;03m# Using ThreadPoolExecutor to parallelize the loop\u001b[39;00m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ThreadPoolExecutor(max_workers\u001b[38;5;241m=\u001b[39mmax_workers) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[1;32m     94\u001b[0m         \u001b[38;5;66;03m# Map the worker function to selected_prompts and collect results\u001b[39;00m\n\u001b[0;32m---> 95\u001b[0m         translated_codes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexecutor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprocess_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselected_prompts\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTranslated \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(translated_codes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m prompts in parallel\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/_base.py:619\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[1;32m    618\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 619\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_result_or_cancel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    620\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    621\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs\u001b[38;5;241m.\u001b[39mpop(), end_time \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic())\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/_base.py:317\u001b[0m, in \u001b[0;36m_result_or_cancel\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 317\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    319\u001b[0m         fut\u001b[38;5;241m.\u001b[39mcancel()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 451\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/threading.py:355\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 355\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    356\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(f'{cwd}/../../../')\n",
    "\n",
    "from bigcode_eval.tasks.code_to_code.codegeex_chat_pro import codegeex_chat_pro_translate_and_postprocess\n",
    "\n",
    "prompt_version='v2'\n",
    "\n",
    "translated_prompts_path = f'{cwd}/../../datasets/humaneval-x/humaneval_python_java_prompts_{prompt_version}.json'\n",
    "\n",
    "limit_start = 0\n",
    "limit = len(json.load(open(translated_prompts_path, \"r\")))\n",
    "model = 'codegeex-chat-pro'\n",
    "\n",
    "save_translated_codes_path = f'{cwd}/{model}-humaneval_python_java_prompts_{prompt_version}-translations-{limit_start}-{limit}.json'\n",
    "\n",
    "codegeex_chat_pro_translate_and_postprocess(\n",
    "    translated_prompts_path, save_translated_codes_path, limit_start, limit, parallel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Convert Python to Java code step-by-step starting with the predefined template:\n",
    "Java:\n",
    "```\n",
    "import java.lang.*;\n",
    "import java.util.*;\n",
    "\n",
    "class Solution {\n",
    "    public List <String> separateParenGroups(String paren_string) {\n",
    "```\n",
    "\n",
    "Python:\n",
    "```from typing import List\n",
    "\n",
    "def separate_paren_groups(paren_string: str) -> List[str]:\n",
    "    result = []\n",
    "    current_string = []\n",
    "    current_depth = 0\n",
    "\n",
    "    for c in paren_string:\n",
    "        if c == '(':\n",
    "            current_depth += 1\n",
    "            current_string.append(c)\n",
    "        elif c == ')':\n",
    "            current_depth -= 1\n",
    "            current_string.append(c)\n",
    "\n",
    "            if current_depth == 0:\n",
    "                result.append(''.join(current_string))\n",
    "                current_string.clear()\n",
    "\n",
    "    return result\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(f'{cwd}/../../../')\n",
    "\n",
    "from bigcode_eval.tasks.code_to_code.codegeex_chat_pro import codegeex_chat_pro_translate\n",
    "\n",
    "print(codegeex_chat_pro_translate(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(f'{cwd}/../../../')\n",
    "\n",
    "from bigcode_eval.tasks.code_to_code.codegeex_chat_pro import codegeex_chat_pro_translate_and_postprocess\n",
    "\n",
    "prompt_version='v3'\n",
    "\n",
    "translated_prompts_path = f'{cwd}/../../datasets/humaneval-x/humaneval_python_java_prompts_{prompt_version}.json'\n",
    "\n",
    "limit_start = 0\n",
    "limit = len(json.load(open(translated_prompts_path, \"r\")))\n",
    "model = 'codegeex-chat-pro'\n",
    "\n",
    "save_translated_codes_path = f'{cwd}/{model}-humaneval_python_java_prompts_{prompt_version}-translations-{limit_start}-{limit}.json'\n",
    "\n",
    "codegeex_chat_pro_translate_and_postprocess(\n",
    "    translated_prompts_path, save_translated_codes_path, limit_start, limit, parallel=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using parallel processing with #5 workers\n",
      "Translated 164 prompts in parallel\n",
      "saved #164 translations to /Users/phuonglvh/projects/2170558-thesis-automatic-code-generation-using-machine-learning/bigcode-evaluation-harness/benchmark/codegeex-chat-pro/humaneval-x/codegeex-chat-pro-humaneval_python_java_prompts_v4.1-translations-0-164.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(f'{cwd}/../../../')\n",
    "\n",
    "from bigcode_eval.tasks.code_to_code.codegeex_chat_pro import codegeex_chat_pro_translate_and_postprocess\n",
    "\n",
    "prompt_version = 'v4'\n",
    "# prompt_version = 'v4.1'\n",
    "\n",
    "translated_prompts_path = f'{cwd}/../../datasets/humaneval-x/humaneval_python_java_prompts_{prompt_version}.json'\n",
    "\n",
    "limit_start = 0\n",
    "limit = len(json.load(open(translated_prompts_path, \"r\")))\n",
    "model = 'codegeex-chat-pro'\n",
    "\n",
    "save_translated_codes_path = f'{cwd}/{model}-humaneval_python_java_prompts_{prompt_version}-translations-{limit_start}-{limit}.json'\n",
    "\n",
    "codegeex_chat_pro_translate_and_postprocess(\n",
    "    translated_prompts_path, save_translated_codes_path, limit_start, limit, parallel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using parallel processing with #5 workers\n",
      "Translated 164 prompts in parallel\n",
      "saved #164 translations to /Users/phuonglvh/projects/2170558-thesis-automatic-code-generation-using-machine-learning/bigcode-evaluation-harness/benchmark/codegeex-chat-pro/humaneval-x/codegeex-chat-pro-humaneval_python_java_prompts_v5.1-translations-0-164.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(f'{cwd}/../../../')\n",
    "\n",
    "from bigcode_eval.tasks.code_to_code.codegeex_chat_pro import codegeex_chat_pro_translate_and_postprocess\n",
    "\n",
    "# prompt_version='v5'\n",
    "prompt_version='v5.1'\n",
    "\n",
    "translated_prompts_path = f'{cwd}/../../datasets/humaneval-x/humaneval_python_java_prompts_{prompt_version}.json'\n",
    "\n",
    "limit_start = 0\n",
    "limit = len(json.load(open(translated_prompts_path, \"r\")))\n",
    "model = 'codegeex-chat-pro'\n",
    "\n",
    "save_translated_codes_path = f'{cwd}/{model}-humaneval_python_java_prompts_{prompt_version}-translations-{limit_start}-{limit}.json'\n",
    "\n",
    "codegeex_chat_pro_translate_and_postprocess(\n",
    "    translated_prompts_path, save_translated_codes_path, limit_start, limit, parallel=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v6=v5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using parallel processing with #5 workers\n",
      "Translated 164 prompts in parallel\n",
      "saved #164 translations to /Users/phuonglvh/projects/2170558-thesis-automatic-code-generation-using-machine-learning/bigcode-evaluation-harness/benchmark/codegeex-chat-pro/humaneval-x/codegeex-chat-pro-humaneval_python_java_prompts_v6-translations-0-164.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(f'{cwd}/../../../')\n",
    "\n",
    "from bigcode_eval.tasks.code_to_code.codegeex_chat_pro import codegeex_chat_pro_translate_and_postprocess\n",
    "\n",
    "prompt_version='v6'\n",
    "\n",
    "translated_prompts_path = f'{cwd}/../../datasets/humaneval-x/humaneval_python_java_prompts_{prompt_version}.json'\n",
    "\n",
    "limit_start = 0\n",
    "limit = len(json.load(open(translated_prompts_path, \"r\")))\n",
    "model = 'codegeex-chat-pro'\n",
    "\n",
    "save_translated_codes_path = f'{cwd}/{model}-humaneval_python_java_prompts_{prompt_version}-translations-{limit_start}-{limit}.json'\n",
    "\n",
    "codegeex_chat_pro_translate_and_postprocess(\n",
    "    translated_prompts_path, save_translated_codes_path, limit_start, limit, parallel=True, max_workers=5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigcode-evaluation-harness",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
